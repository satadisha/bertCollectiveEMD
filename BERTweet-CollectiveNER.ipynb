{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1614199650915,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "rrLFVF0oiBlh",
    "outputId": "05868674-3783-43a1-d7c5-3fbc2c680681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.3\r\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1920,
     "status": "ok",
     "timestamp": 1614199652463,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "V3WDvVQhayIZ",
    "outputId": "0644ccfa-9f99-423d-eae4-ec38bbae79ee"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive',force_remount=True)\n",
    "# %cd gdrive/My Drive/BERTweet-ner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9676,
     "status": "ok",
     "timestamp": 1614199660283,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "AEvo5aXwYILN",
    "outputId": "21f0756d-9472-4dbf-9184-a2a88275b528"
   },
   "outputs": [],
   "source": [
    "# !pip3 install datasets\n",
    "# !pip3 install transformers\n",
    "# !pip3 install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5893,
     "status": "ok",
     "timestamp": 1614199662951,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "BuR-p1-pMK2Q",
    "outputId": "00cfc0ef-faf1-4430-e941-566b1baad058"
   },
   "outputs": [],
   "source": [
    "# !pip3 install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7392,
     "status": "ok",
     "timestamp": 1614199665605,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "xhAmsA61gP6q"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5930,
     "status": "ok",
     "timestamp": 1614199665605,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "W5tTI3fIl9yC"
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"vinai/bertweet-base\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1614194457119,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "YTip5Uxs29P8"
   },
   "outputs": [],
   "source": [
    "# # reading the training set\n",
    "# f = open(\"wnut17_train.annotated\", \"r\")\n",
    "# file_text=f.read()\n",
    "# df_columns=['id','tokens','ner_tags']\n",
    "# df_holder=[]\n",
    "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
    "\n",
    "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
    "    \n",
    "#     words=[]\n",
    "#     annotations=[]\n",
    "#     lines=bio_sentence.split('\\n')\n",
    "\n",
    "#     for line in lines:\n",
    "#         if(line):\n",
    "#             tabs=line.split('\\t')\n",
    "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
    "#                 word=tabs[0]\n",
    "#                 tag=tabs[1]\n",
    "#                 # if('-' not in tabs[1]):\n",
    "#                 #     tag=tabs[1]\n",
    "#                 # else:\n",
    "#                 #     tag=tabs[1].split('-')[0]\n",
    "#                 if(word.strip().startswith('https:')):\n",
    "#                     word='@url'\n",
    "#                 words.append(word.strip())\n",
    "#                 annotations.append(tag)\n",
    "\n",
    "#     # text=words.strip()\n",
    "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
    "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
    "#     df_holder.append(df_dict)\n",
    "# df_train = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print(len(df_train))\n",
    "# df_train.to_csv(\"wnut17train.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "# # reading the validation set\n",
    "# f = open(\"wnut17_validation.annotated\", \"r\")\n",
    "# file_text=f.read()\n",
    "# df_columns=['id','tokens','ner_tags']\n",
    "# df_holder=[]\n",
    "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
    "\n",
    "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
    "    \n",
    "#     words=[]\n",
    "#     annotations=[]\n",
    "#     lines=bio_sentence.split('\\n')\n",
    "\n",
    "#     for line in lines:\n",
    "#         if(line):\n",
    "#             tabs=line.split('\\t')\n",
    "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
    "#                 word=tabs[0]\n",
    "#                 tag=tabs[1]\n",
    "#                 # if('-' not in tabs[1]):\n",
    "#                 #     tag=tabs[1]\n",
    "#                 # else:\n",
    "#                 #     tag=tabs[1].split('-')[0]\n",
    "#                 if(word.strip().startswith('https:')):\n",
    "#                     word='@url'\n",
    "#                 words.append(word.strip())\n",
    "#                 annotations.append(tag)\n",
    "\n",
    "#     # text=words.strip()\n",
    "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
    "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
    "#     df_holder.append(df_dict)\n",
    "# df_validation = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print(len(df_validation))\n",
    "# df_validation.to_csv(\"wnut17validation.csv\", sep=',', encoding='utf-8',index=False)\n",
    "\n",
    "# # reading the test set\n",
    "# f = open(\"wnut17_test.annotated\", \"r\")\n",
    "# file_text=f.read()\n",
    "# df_columns=['id','tokens','ner_tags']\n",
    "# df_holder=[]\n",
    "# bio_sentences=list(filter (lambda elem: elem!='', file_text.split('\\n\\n')))\n",
    "\n",
    "# for bio_id, bio_sentence in enumerate(bio_sentences):\n",
    "    \n",
    "#     words=[]\n",
    "#     annotations=[]\n",
    "#     lines=bio_sentence.split('\\n')\n",
    "\n",
    "#     for line in lines:\n",
    "#         if(line):\n",
    "#             tabs=line.split('\\t')\n",
    "#             if not((tabs[0]=='')&(tabs[1]=='')):\n",
    "#                 word=tabs[0]\n",
    "#                 tag=tabs[1]\n",
    "#                 # if('-' not in tabs[1]):\n",
    "#                 #     tag=tabs[1]\n",
    "#                 # else:\n",
    "#                 #     tag=tabs[1].split('-')[0]\n",
    "#                 if(word.strip().startswith('https:')):\n",
    "#                     word='@url'\n",
    "#                 words.append(word.strip())\n",
    "#                 annotations.append(tag)\n",
    "\n",
    "#     # text=words.strip()\n",
    "#     # annotation=','.join(get_entities(word_tag_tuples_annotation))\n",
    "#     df_dict={'ID':str(bio_id),'tokens':words,'ner_tags':annotations}\n",
    "#     df_holder.append(df_dict)\n",
    "# df_test = pd.DataFrame(df_holder,columns=df_columns)\n",
    "# print(len(df_test))\n",
    "# df_test.to_csv(\"wnut17test.csv\", sep=',', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2891,
     "status": "ok",
     "timestamp": 1614199666713,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "WDmC8lnI-99G",
    "outputId": "d5534d37-1518-49a5-b7c1-4ae0a87e9fad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wnut_17 (/home/satadisha/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_dataset = load_dataset('csv', data_files=\"wnut17train.csv\")\n",
    "# validation_dataset = load_dataset('csv', data_files='wnut17validation.csv')\n",
    "# test_dataset = load_dataset('csv', data_files=\"wnut17test.csv\")\n",
    "\n",
    "datasets = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1614199668013,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "3Z17jPrtnKuu",
    "outputId": "44a621f5-6f69-41f0-97ba-096bf9afa256"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_dataset\n",
    "datasets\n",
    "# train_dataset[\"train\"].features[f\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1614199671426,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "bkvoRJbVbIeX",
    "outputId": "53e598df-b1cc-4df8-9f40-1b0969a6b70d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False, normalization=True)\n",
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "csG7slfdkVwg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 950,
     "status": "ok",
     "timestamp": 1614199672765,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "yyth64PVbIht"
   },
   "outputs": [],
   "source": [
    "expanded_label_dict={0:'O', 1:'B-corporation', 2:'I-corporation', 3:'B-creative-work', 4:'I-creative-work', 5:'B-group', 6:'I-group', 7:'B-location', 8:'I-location', 9:'B-person', 10:'I-person', 11:'B-product', 12:'I-product'}\n",
    "BIO_dict={'O':0,'B':1,'I':2}\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "        \n",
    "    tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "    inputId_to_token_dict={}\n",
    "    for index, token in enumerate(example[\"tokens\"]):\n",
    "        values=tokenizer.encode(token, add_special_tokens=False, truncation=True)\n",
    "        for value in values:\n",
    "            try:\n",
    "                inputId_to_token_dict[value].append(index)\n",
    "            except KeyError:\n",
    "                inputId_to_token_dict[value]=[index]\n",
    "    labels=[]\n",
    "    for inputID in tokenized_input['input_ids']:\n",
    "        try:\n",
    "            index_list=copy.deepcopy(inputId_to_token_dict[inputID])\n",
    "            index_to_address=index_list.pop(0)\n",
    "\n",
    "            # label=BIO_dict[expanded_label_dict[example['ner_tags'][index_to_address]][0]] #Just BIO\n",
    "            label = example['ner_tags'][index_to_address]\n",
    "\n",
    "            labels.append(label)\n",
    "            inputId_to_token_dict[inputID]=index_list\n",
    "        except KeyError:\n",
    "            labels.append(-100)\n",
    "\n",
    "    assert (len(tokenized_input['input_ids']) == len(labels))\n",
    "    tokenized_input['labels']=labels\n",
    "    \n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132,
     "referenced_widgets": [
      "b67d2a27c9ad4b018903a0a7afde3a20",
      "56acf1016e8b40289555396d6ab3dee7",
      "c3e04128e7cd488e801c8a31b17edce3",
      "48448e592c5b4d2190545276aa136f99",
      "cc7665ab60314474b1b2f38639a8986c",
      "485e8c2ad26145e68ae482d8206cf3f8",
      "99f43041f80e41bea13dd071f1c24a8b",
      "d4fc8c07114a4b658bb555c955334f19",
      "3a0681b5ea5b4ec1903e7584079c1f5d",
      "66e745e8a80545259a5e9bfaf82807a3",
      "5cf84f3ac4564b2da0435adbbfe742c4",
      "e00e9ad5eb384df996b593e6f8880164",
      "16630435853948fb8577359df4fced07",
      "9f4914e1c3be4c258ac0d3b35a3fb2d9",
      "f9dfec3fd9034e0e8ee23489f5331bbf",
      "dd181fcce48041f4b634d2e562bf3b43"
     ]
    },
    "executionInfo": {
     "elapsed": 12002,
     "status": "ok",
     "timestamp": 1614199687029,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "6q5SzJpD4cId",
    "outputId": "f36903ac-9263-4eb3-d7e1-040a6d3a023c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/satadisha/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8/cache-b3a2fe4ee0a9800e.arrow\n",
      "Loading cached processed dataset at /home/satadisha/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8/cache-7dcdfcc9a1df4880.arrow\n",
      "Loading cached processed dataset at /home/satadisha/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/983205ce50100b6e8fce4b3d402f36dce9b206f736e1a630c78fb25e1d23b9e8/cache-71cebb5261967793.arrow\n"
     ]
    }
   ],
   "source": [
    "# tokenize_and_align_labels(datasets['train'][2])\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10029,
     "status": "ok",
     "timestamp": 1614199687031,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "XthRN5iUh_Kq",
    "outputId": "7b57c25f-7f47-47dc-8ef8-ded1b5302519"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'token_type_ids', 'tokens'],\n",
       "    num_rows: 3394\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7716,
     "status": "ok",
     "timestamp": 1614199689660,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "ZUz5eC7SfFqc",
    "outputId": "39eb1e08-84b3-4a68-82ae-f2f37d3b0a6d"
   },
   "outputs": [],
   "source": [
    "# !pip3 install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 7066,
     "status": "ok",
     "timestamp": 1614199690480,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "FLWFWwewCBjL"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Xhh4V16GoHKx"
   },
   "outputs": [],
   "source": [
    "# tokenized_datasets['train'].features[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1298,
     "status": "ok",
     "timestamp": 1614199691839,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "xNzwMflzkDi3",
    "outputId": "bff3c7a0-354a-4331-8a38-ede2b8eeb2ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "\n",
    "# label_list = ['O','B','I']\n",
    "\n",
    "print(label_list)\n",
    "print(len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1614199691840,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "PV4eL-hsIafT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # print(p.shape)\n",
    "    output, labels = p\n",
    "\n",
    "    # print(len(predictions))\n",
    "    # print(predictions[0].shape)\n",
    "    # for elem in predictions[1]:\n",
    "    #   print(elem.shape)\n",
    "\n",
    "#     predictions, _ = output\n",
    "    predictions = output[0]\n",
    "    \n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "FNcDvqvUkgkc"
   },
   "outputs": [],
   "source": [
    "# config = AutoConfig.from_pretrained(model_checkpoint, output_hidden_states=True)\n",
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5951,
     "status": "ok",
     "timestamp": 1614199696531,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "H9akEbBY2ZZl",
    "outputId": "d45c11fd-2154-4d49-9dd5-d89020257f14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"vinai/bertweet-base\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\",\n",
       "    \"7\": \"LABEL_7\",\n",
       "    \"8\": \"LABEL_8\",\n",
       "    \"9\": \"LABEL_9\",\n",
       "    \"10\": \"LABEL_10\",\n",
       "    \"11\": \"LABEL_11\",\n",
       "    \"12\": \"LABEL_12\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_10\": 10,\n",
       "    \"LABEL_11\": 11,\n",
       "    \"LABEL_12\": 12,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6,\n",
       "    \"LABEL_7\": 7,\n",
       "    \"LABEL_8\": 8,\n",
       "    \"LABEL_9\": 9\n",
       "  },\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"max_position_embeddings\": 130,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"output_hidden_states\": true,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
       "  \"transformers_version\": \"4.3.3\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 64001\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_model = AutoModelForTokenClassification.from_pretrained(\"vinai/bertweet-base\", output_hidden_states=True, num_labels=len(label_list))\n",
    "alt_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3369,
     "status": "ok",
     "timestamp": 1614199696532,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "NRnD-mNZUaia",
    "outputId": "47266418-6cf6-45ed-b37c-eb90875707b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArguments(output_dir=test-ner, overwrite_output_dir=False, do_train=False, do_eval=None, do_predict=False, evaluation_strategy=EvaluationStrategy.EPOCH, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=16, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=2e-05, weight_decay=0.01, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_steps=0, logging_dir=runs/Mar01_17-15-36_meng-nlp, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=test-ner, disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=False, deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, _n_gpu=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_training_args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "alt_training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 5165,
     "status": "ok",
     "timestamp": 1614199700230,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "aACODJ6AWVDG"
   },
   "outputs": [],
   "source": [
    "alt_trainer = Trainer(\n",
    "    alt_model,\n",
    "    alt_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gxw-OLa6mitu"
   },
   "source": [
    "**Model output is a tuple, when all hidden states are returned, i.e. output_hidden_states =True in config: (output, hidden-layers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 55541,
     "status": "error",
     "timestamp": 1614199752488,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "-Uryn4hHX06d",
    "outputId": "b12c01e2-947a-40d0-c8ce-b50eed7859a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='639' max='639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [639/639 01:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.361276</td>\n",
       "      <td>0.616438</td>\n",
       "      <td>0.261845</td>\n",
       "      <td>0.367561</td>\n",
       "      <td>0.921677</td>\n",
       "      <td>4.770100</td>\n",
       "      <td>211.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.657635</td>\n",
       "      <td>0.443890</td>\n",
       "      <td>0.530025</td>\n",
       "      <td>0.937080</td>\n",
       "      <td>3.320100</td>\n",
       "      <td>303.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.273000</td>\n",
       "      <td>0.289520</td>\n",
       "      <td>0.668246</td>\n",
       "      <td>0.468828</td>\n",
       "      <td>0.551050</td>\n",
       "      <td>0.940351</td>\n",
       "      <td>3.363300</td>\n",
       "      <td>300.002000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/satadisha/.local/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=639, training_loss=0.24523259850921392, metrics={'train_runtime': 63.3385, 'train_samples_per_second': 10.089, 'total_flos': 309189764708700, 'epoch': 3.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvEaaQhsX8aG"
   },
   "source": [
    "## **Initial Trainer with Default Config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "2NDFUYBfCEwG"
   },
   "outputs": [],
   "source": [
    "# labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "# metric.compute(predictions=[labels], references=[labels])\n",
    "\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "# model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=3) #Just BIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7abGq0PXk040"
   },
   "outputs": [],
   "source": [
    "# args = TrainingArguments(\n",
    "#     f\"test-{task}\",\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     learning_rate=2e-5,\n",
    "#     per_device_train_batch_size=batch_size,\n",
    "#     per_device_eval_batch_size=batch_size,\n",
    "#     num_train_epochs=3,\n",
    "#     weight_decay=0.01,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1p7ArnENIfTE"
   },
   "outputs": [],
   "source": [
    "# trainer = Trainer(\n",
    "#     model,\n",
    "#     args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=tokenizer,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "0K5YzgeKIlzj"
   },
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "executionInfo": {
     "elapsed": 943,
     "status": "error",
     "timestamp": 1613610663265,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggaia4ejPlRMt1Evm_RnhjlvN30MHn4iSZAd_Pm=s64",
      "userId": "17980263953242551956"
     },
     "user_tz": 300
    },
    "id": "Hscli_TYIq1c",
    "outputId": "4301067d-9e3f-4b4a-85d1-c6f03ed9444e"
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCq9EEmjU1li"
   },
   "source": [
    "## **Predict on validation/test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dTyESmmTIzzM"
   },
   "outputs": [],
   "source": [
    "# predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "# predictions = np.argmax(predictions, axis=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "executionInfo": {
     "elapsed": 13565,
     "status": "ok",
     "timestamp": 1614158779032,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "KYKXZlTesZAI",
    "outputId": "a131394e-53ce-4b80-a12b-cca7b2ace55a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_returns, labels, _ = alt_trainer.predict(tokenized_datasets[\"test\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yqXzfKOmvT8Y"
   },
   "outputs": [],
   "source": [
    "# predictions, _ = model_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1614158790451,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "th4654zaJCNA",
    "outputId": "9fa5909d-43bd-44dd-f254-00faf6ca673a"
   },
   "outputs": [],
   "source": [
    "# predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# # Remove ignored index (special tokens)\n",
    "# true_predictions = [\n",
    "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "# true_labels = [\n",
    "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "\n",
    "# results = metric.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 571,
     "status": "ok",
     "timestamp": 1614158793780,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "bYiQSUcuLKXb",
    "outputId": "ff13bda4-a601-4b69-f132-235d4c211dbd"
   },
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "BzbUNafHt3kj"
   },
   "outputs": [],
   "source": [
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "v8aiYWi6B5X6"
   },
   "outputs": [],
   "source": [
    "# # print(predictions[:5])\n",
    "# # Remove ignored index (special tokens)\n",
    "# true_predictions = [\n",
    "#     [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "\n",
    "# true_labels = [\n",
    "#     [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "#     for prediction, label in zip(predictions, labels)\n",
    "# ]\n",
    "\n",
    "# print(true_predictions[:5])\n",
    "# print(true_labels[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wktgUaW8zVl"
   },
   "outputs": [],
   "source": [
    "# results = metric.compute(predictions=true_predictions, references=true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rmCDlyQsoLJY"
   },
   "outputs": [],
   "source": [
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9qob4MoKoXgs"
   },
   "outputs": [],
   "source": [
    "def collate_token_labels(token_dict, prediction_labels):\n",
    "    counter=0\n",
    "    collated_labels=[]\n",
    "    for key in token_dict.keys():\n",
    "        vals=token_dict[key]\n",
    "        labels=prediction_labels[counter:counter+len(vals)]\n",
    "        if('I' in labels):\n",
    "            collated_labels.append('I')\n",
    "        elif('B' in labels):\n",
    "            collated_labels.append('B')\n",
    "        else:\n",
    "            collated_labels.append('O')\n",
    "        counter+=len(vals)\n",
    "    return collated_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_token_label_embedding(token_dict, prediction_labels, entity_embeddings):\n",
    "    counter=0\n",
    "    collated_labels=[]\n",
    "    collated_entity_embeddings=[]\n",
    "    for key in token_dict.keys():\n",
    "        vals=token_dict[key]\n",
    "        labels=prediction_labels[counter:counter+len(vals)]\n",
    "        token_entity_embeddings=entity_embeddings[counter:counter+len(vals)]\n",
    "#         print(token_entity_embeddings.shape)\n",
    "        collated_entity_embeddings.append(torch.mean(token_entity_embeddings,dim=0))\n",
    "#         print(collated_entity_embeddings)\n",
    "        if('I' in labels):\n",
    "            collated_labels.append('I')\n",
    "        elif('B' in labels):\n",
    "            collated_labels.append('B')\n",
    "        else:\n",
    "            collated_labels.append('O')\n",
    "        counter+=len(vals)\n",
    "    assert len(collated_labels)==len(collated_entity_embeddings)\n",
    "    return collated_labels,collated_entity_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm1P-HYgU6kb"
   },
   "source": [
    "## **Pre-processing on custom test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1204,
     "status": "ok",
     "timestamp": 1614158872756,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "XgRPt985oiDa",
    "outputId": "5f879bf6-2f71-4cd8-8725-aa952c269630"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "safU-gd3oaRx"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktTrainer\n",
    "import string\n",
    "\n",
    "string.punctuation=string.punctuation+'…‘’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgl7fxwtSnHO"
   },
   "outputs": [],
   "source": [
    "def get_entities(word_tag_tuples):\n",
    "    \n",
    "    mentions=[]\n",
    "    candidateMention=''\n",
    "    #emoji.get_emoji_regexp().sub(u'', candidateMention)\n",
    "    for tup in word_tag_tuples:\n",
    "        candidate=tup[0]\n",
    "        tag=tup[1]\n",
    "        if(tag=='O'):\n",
    "            if(candidateMention):\n",
    "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "                    if mention_to_add.endswith(\"'s\"):\n",
    "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    elif mention_to_add.endswith(\"’s\"):\n",
    "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    else:\n",
    "                        mention_to_add=mention_to_add\n",
    "                    if(mention_to_add!=''):\n",
    "                        mentions.append(mention_to_add)\n",
    "            candidateMention=''\n",
    "        else:\n",
    "            if (tag=='B'):\n",
    "                if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "                    mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "                    if mention_to_add.endswith(\"'s\"):\n",
    "                        li = mention_to_add.rsplit(\"'s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    elif mention_to_add.endswith(\"’s\"):\n",
    "                        li = mention_to_add.rsplit(\"’s\", 1)\n",
    "                        mention_to_add=''.join(li)\n",
    "                    else:\n",
    "                        mention_to_add=mention_to_add\n",
    "                    if(mention_to_add!=''):\n",
    "                        mentions.append(mention_to_add)\n",
    "                candidateMention=candidate\n",
    "            else:\n",
    "                candidateMention+=\" \"+candidate\n",
    "        # if (tag=='B'):\n",
    "        #     if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))):\n",
    "        #         mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "        #         if(mention_to_add):\n",
    "        #             mentions.append(mention_to_add)\n",
    "        #     candidateMention=candidate\n",
    "        # else:\n",
    "        #     candidateMention+=\" \"+candidate\n",
    "    if(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).strip()):\n",
    "        if((not candidateMention.strip().startswith('#'))&(not candidateMention.strip().startswith('@'))&(not candidateMention.strip().startswith('https:'))):\n",
    "            mention_to_add=emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip()\n",
    "            if(mention_to_add!=''):\n",
    "                mentions.append(mention_to_add)\n",
    "        # mentions.append(emoji.get_emoji_regexp().sub(u'', candidateMention).strip(string.punctuation).lower().strip())\n",
    "    # print('extracted mentions:', mentions)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YqhxNBPM67V"
   },
   "outputs": [],
   "source": [
    "def get_encoding_seq(tweet_word_list, mentions):\n",
    "    print(tweet_word_list)\n",
    "    print(mentions)\n",
    "    tweet_word_index=0\n",
    "    encoded_tag_sequence=[]\n",
    "    while(mentions):\n",
    "        current_mention=[token.strip() for token in mentions.pop(0).split(' ')]\n",
    "        while(normalize(current_mention[0])!=normalize(tweet_word_list[tweet_word_index])):\n",
    "            encoded_tag_sequence.append('O')\n",
    "            tweet_word_index+=1\n",
    "        if(normalize(current_mention[0])==normalize(tweet_word_list[tweet_word_index])):\n",
    "            for token_index, token in enumerate(current_mention):\n",
    "                if(token_index==0):\n",
    "                    encoded_tag_sequence.append('B')\n",
    "                else:\n",
    "                    encoded_tag_sequence.append('I')\n",
    "                tweet_word_index+=1\n",
    "    while(tweet_word_index<len(tweet_word_list)):\n",
    "        encoded_tag_sequence.append('O')\n",
    "        tweet_word_index+=1\n",
    "        \n",
    "    print(encoded_tag_sequence)\n",
    "    return encoded_tag_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4dCD4GDmgnM"
   },
   "outputs": [],
   "source": [
    "gutenberg_text = \"\"\n",
    "for file_id in gutenberg.fileids():\n",
    "    gutenberg_text += gutenberg.raw(file_id)\n",
    "tokenizer_trainer = PunktTrainer()\n",
    "tokenizer_trainer.INCLUDE_ALL_COLLOCS = True\n",
    "tokenizer_trainer.train(gutenberg_text)\n",
    "\n",
    "my_sentence_tokenizer = PunktSentenceTokenizer(tokenizer_trainer.get_params())\n",
    "my_sentence_tokenizer._params.abbrev_types.add('dr')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('c.j')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s')\n",
    "my_sentence_tokenizer._params.abbrev_types.add('u.s.a')\n",
    "\n",
    "def normalize_to_sentences(text):\n",
    "    tweetSentences=list(filter (lambda sentence: len(sentence)>1, text.split('\\n')))\n",
    "    tweetSentenceList_inter=custom_flatten(list(map(lambda sentText: my_sentence_tokenizer.tokenize(sentText.lstrip().rstrip()),tweetSentences)),[])\n",
    "    tweetSentenceList=list(filter (lambda sentence: len(sentence)>1, tweetSentenceList_inter))\n",
    "    return tweetSentenceList\n",
    "\n",
    "def custom_flatten(mylist, outlist,ignore_types=(str, bytes, int)):\n",
    "    \n",
    "    if (mylist !=[]):\n",
    "        for item in mylist:\n",
    "            #print not isinstance(item, ne.NE_candidate)\n",
    "            if isinstance(item, list) and not isinstance(item, ignore_types):\n",
    "                custom_flatten(item, outlist)\n",
    "            else:\n",
    "                item=item.strip(' \\t\\n\\r')\n",
    "                outlist.append(item)\n",
    "    return outlist\n",
    "\n",
    "def preprocess(filename):\n",
    "    \"\"\"save a file with token, label and prediction in each row\"\"\"\n",
    "    tweet_to_sentences_w_annotation={}\n",
    "    sentenceID=0\n",
    "    test=pd.read_csv(\"data/\"+filename,sep =',',keep_default_na=False)\n",
    "    # outputfilename=\"data/covid/covid_2K.txt\"\n",
    "    \n",
    "    all_annotated_ne=[]\n",
    "    tweetsentences=[]\n",
    "    tokenizedsentences=[]\n",
    "    \n",
    "    for row in test.itertuples():\n",
    "        tweetID=str(row.Index)\n",
    "        text=str(row.TweetText)\n",
    "        row_sentences = normalize_to_sentences(text)\n",
    "        tweetsentences += row_sentences\n",
    "        tokenizedsentences += [tokenizer(sentence, is_split_into_words=True) for sentence in row_sentences]\n",
    "        # print(text)\n",
    "        \n",
    "        mentions=[]\n",
    "        for sentence_level in str(row.mentions_other).split(';'):\n",
    "            if(sentence_level):\n",
    "                for mention in sentence_level.split(','):\n",
    "                    if(mention):\n",
    "                        mentions.append(mention.lower().strip(string.punctuation).strip())\n",
    "        mentions=list(filter(lambda element: ((element !='')&(element !='nan')), mentions))\n",
    "        # all_annotated_ne.extend(mentions)\n",
    "        \n",
    "        # if(row_sentences):\n",
    "        tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+len(row_sentences)),mentions)\n",
    "        sentenceID+=len(row_sentences)\n",
    "        # else:\n",
    "        #     tweet_to_sentences_w_annotation[tweetID]=((sentenceID,sentenceID+1),mentions)\n",
    "        #     sentenceID+=1\n",
    "        # print(sentenceID,len(row_sentences))\n",
    "    return tweetsentences, tokenizedsentences, tweet_to_sentences_w_annotation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ep9TRivf7R9-"
   },
   "outputs": [],
   "source": [
    "def calculate_f1(tweet_to_sentences_w_annotation, tweetsentences, true_predictions):\n",
    "    \n",
    "    # dataset, i = [], 0\n",
    "    ner_arrays=[]\n",
    "    file_write_text=''\n",
    "    all_detected_ne=[]\n",
    "    all_annotated_ne=[]\n",
    "    \n",
    "    for n, tweet in enumerate(tweetsentences):\n",
    "        # tweet_data = list(zip(tweet, true_labels[n], predictions[i:i + len(tweet)]))\n",
    "        # word_tag_tuples=zip(tweet,predictions[i:i + len(tweet)])\n",
    "        \n",
    "        assert (len(tweet)==len(true_predictions[n]))\n",
    "        word_tag_tuples=zip(tweet,true_predictions[n])\n",
    "        entities_from_sentence=get_entities(word_tag_tuples)\n",
    "        # line_text='\\t'.join(entities_from_sentence)\n",
    "        # file_write_text+=line_text+'\\n'\n",
    "        # print(entities_from_sentence)\n",
    "        all_detected_ne.extend(entities_from_sentence)\n",
    "        ner_arrays.append(entities_from_sentence)\n",
    "        # dataset += tweet_data + [()]\n",
    "    \n",
    "    print('tally:',len(tweetsentences),len(ner_arrays))\n",
    "    system_output_mention_list=list(set(all_detected_ne))\n",
    "    # file_write_text='\\n'.join(system_output_mention_list)\n",
    "    # f1= open(outputfilename, \"w\")\n",
    "    # f1.write(file_write_text)\n",
    "    # f1.close()\n",
    "    \n",
    "    true_positive_count=0\n",
    "    false_positive_count=0\n",
    "    false_negative_count=0\n",
    "    total_mentions=0\n",
    "    total_annotation=0\n",
    "    \n",
    "    \n",
    "    for tweetID in tweet_to_sentences_w_annotation.keys():\n",
    "        unrecovered_annotated_mention_list=[]\n",
    "        tp_counter_inner=0\n",
    "        fp_counter_inner=0\n",
    "        fn_counter_inner=0\n",
    "        \n",
    "        annotated_mention_list=tweet_to_sentences_w_annotation[tweetID][1]\n",
    "        all_annotated_ne.extend(annotated_mention_list)\n",
    "        output_mentions_list=[]\n",
    "        idRange=tweet_to_sentences_w_annotation[tweetID][0]\n",
    "        for sentID in range(idRange[0],idRange[1]):\n",
    "            output_mentions_list+=ner_arrays[sentID]\n",
    "        print(tweetID,annotated_mention_list,output_mentions_list)\n",
    "        all_postitive_counter_inner=len(output_mentions_list)\n",
    "        while(annotated_mention_list):\n",
    "            if(len(output_mentions_list)):\n",
    "                annotated_candidate= annotated_mention_list.pop()\n",
    "                if(annotated_candidate in output_mentions_list):\n",
    "                    output_mentions_list.pop(output_mentions_list.index(annotated_candidate))\n",
    "                    tp_counter_inner+=1\n",
    "                else:\n",
    "                    unrecovered_annotated_mention_list.append(annotated_candidate)\n",
    "            else:\n",
    "                unrecovered_annotated_mention_list.extend(annotated_mention_list)\n",
    "                break\n",
    "        # unrecovered_annotated_mention_list_outer.extend(unrecovered_annotated_mention_list)\n",
    "        fn_counter_inner=len(unrecovered_annotated_mention_list)\n",
    "        fp_counter_inner=all_postitive_counter_inner - tp_counter_inner\n",
    "        \n",
    "        print(tp_counter_inner,fp_counter_inner,fn_counter_inner)\n",
    "        \n",
    "        true_positive_count+=tp_counter_inner\n",
    "        false_positive_count+=fp_counter_inner\n",
    "        false_negative_count+=fn_counter_inner\n",
    "        \n",
    "    print('true_positive_count,false_positive_count,false_negative_count:')\n",
    "    print(true_positive_count,false_positive_count,false_negative_count)\n",
    "    \n",
    "    precision=(true_positive_count)/(true_positive_count+false_positive_count)\n",
    "    recall=(true_positive_count)/(true_positive_count+false_negative_count)\n",
    "    f_measure=2*(precision*recall)/(precision+recall)\n",
    "            \n",
    "    print('========Entity Mention Detection========')\n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)\n",
    "\n",
    "    print('========Entity Detection========')\n",
    "    true_positive_entities =  len(list(set(all_detected_ne).intersection(set(all_annotated_ne))))\n",
    "    false_positive_entities = len(list(set(all_annotated_ne)-set(all_detected_ne)))\n",
    "    false_negative_entities = len(list(set(all_detected_ne)-set(all_annotated_ne)))\n",
    "\n",
    "    precision= (true_positive_entities)/(true_positive_entities+false_positive_entities)\n",
    "    recall= (true_positive_entities)/(true_positive_entities+false_negative_entities)\n",
    "    f_measure = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    print('precision: ',precision)\n",
    "    print('recall: ',recall)\n",
    "    print('f_measure: ',f_measure)   \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbajzPkDC_nO"
   },
   "outputs": [],
   "source": [
    "# tweets_unpartitoned=pd.read_csv(\"/Users/satadisha/Documents/GitHub/TwiCSv2/data/tweets_3k_annotated.csv\",sep =',',keep_default_na=False)\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('tweets_3k_annotated.csv')\n",
    "\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('venezuela.csv')\n",
    "\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billdeblasio.csv')\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('pikapika.csv')\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('ripcity.csv')\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('billnye.csv')\n",
    "# testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('roevwade.csv')\n",
    "\n",
    "testset, tokenizedtestset, tweet_to_sentences_w_annotation = preprocess('wnut17test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 756,
     "status": "ok",
     "timestamp": 1614158922220,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "01Qa9DQDpWIG",
    "outputId": "a4a23359-f818-4ca4-a1bd-b0915071ee88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1864\n",
      "['& gt ; * The soldier was killed when another avalanche hit an army barracks in the northern area of Sonmarg , said a military spokesman .', '& gt ; * Police last week evacuated 80 villagers from Waltengoo Nar where dozens were killed after a series of avalanches hit the area in 2005 in the south of the territory .', '& gt ; * The army on Thursday recovered the bodies of ten of its men who were killed in an avalanche the previous day .', '& gt ; * The four civilians killed included two children of a family whose house was hit by a separate avalanche , also on Wednesday , a police spokesman said .', 'The bodies of the soldiers were recovered after the concerted efforts of the Avalanche Rescue Teams ( ART ) , which is equipped to work in inhospitable terrain and weather conditions .']\n"
     ]
    }
   ],
   "source": [
    "# tokenized_datasets = testset.map(tokenize_and_align_labels)\n",
    "print(len(testset))\n",
    "print(testset[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rALqGOh7gi5q"
   },
   "outputs": [],
   "source": [
    "# predictions, labels, _ = trainer.predict(tokenizedtestset)\n",
    "# predictions = np.argmax(predictions, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vSKBnXUpwvMH"
   },
   "outputs": [],
   "source": [
    "# print(len(predictions))\n",
    "# print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21518,
     "status": "ok",
     "timestamp": 1614158957446,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "ifPNenaBqUdv",
    "outputId": "9cb333f7-1f74-4dab-85ce-5b5599fdfc25",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "dict_keys(['&', 'gt', ';', '*', 'The', 'soldier', 'was', 'killed', 'when', 'another', 'avalanche', 'hit', 'an', 'army', 'barracks', 'in', 'the', 'northern', 'area', 'of', 'Sonmarg', ',', 'said', 'a', 'military', 'spokesman', '.'])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47, 10998,    38,  1294,    64,\n",
      "           347, 36791, 25313,   512,    74,  3466,  2149, 18549,    16,     6,\n",
      "          7322,  1149,    15,  7725,  2350,   714,     7,   193,    11,  2492,\n",
      "         34196,     4,     2]])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47, 10998,    38,  1294,    64,\n",
      "           347, 36791, 25313,   512,    74,  3466,  2149, 18549,    16,     6,\n",
      "          7322,  1149,    15,  7725,  2350,   714,     7,   193,    11,  2492,\n",
      "         34196,     4,     2]], device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "13\n",
      "dict_keys(['&', 'gt', ';', '*', 'Police', 'last', 'week', 'evacuated', '80', 'villagers', 'from', 'Waltengoo', 'Nar', 'where', 'dozens', 'were', 'killed', 'after', 'a', 'series', 'of', 'avalanches', 'hit', 'the', 'area', 'in', '2005', 'south', 'territory', '.'])\n",
      "tensor([[    0,    55,  8083,   208,   110,  2282,   175,   223, 32138,  1785,\n",
      "         47942,    53,  5047,  4744, 15258,   450,  1348,   209, 19116,   147,\n",
      "          1294,   177,    11,  1049,    15, 36791,   725,  3652,   512,     6,\n",
      "          1149,    16,  3414,    16,     6,  2729,    15,     6, 10909,     4,\n",
      "             2]])\n",
      "tensor([[    0,    55,  8083,   208,   110,  2282,   175,   223, 32138,  1785,\n",
      "         47942,    53,  5047,  4744, 15258,   450,  1348,   209, 19116,   147,\n",
      "          1294,   177,    11,  1049,    15, 36791,   725,  3652,   512,     6,\n",
      "          1149,    16,  3414,    16,     6,  2729,    15,     6, 10909,     4,\n",
      "             2]], device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "13\n",
      "dict_keys(['&', 'gt', ';', '*', 'The', 'army', 'on', 'Thursday', 'recovered', 'the', 'bodies', 'of', 'ten', 'its', 'men', 'who', 'were', 'killed', 'in', 'an', 'avalanche', 'previous', 'day', '.'])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47,  3466,    24,  1638, 14595,\n",
      "             6,  5644,    15,  2540,    15,   139,   656,    87,   147,  1294,\n",
      "            16,    74, 36791, 25313,     6,  4006,    93,     4,     2]])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47,  3466,    24,  1638, 14595,\n",
      "             6,  5644,    15,  2540,    15,   139,   656,    87,   147,  1294,\n",
      "            16,    74, 36791, 25313,     6,  4006,    93,     4,     2]],\n",
      "       device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "13\n",
      "dict_keys(['&', 'gt', ';', '*', 'The', 'four', 'civilians', 'killed', 'included', 'two', 'children', 'of', 'a', 'family', 'whose', 'house', 'was', 'hit', 'by', 'separate', 'avalanche', ',', 'also', 'on', 'Wednesday', 'police', 'spokesman', 'said', '.'])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47,  1362, 17260,  1294,  3280,\n",
      "           255,   994,    15,    11,   383,  3430,   364,    38,   512,    61,\n",
      "            11,  5628, 36791, 25313,     7,   237,    24,  1895,     7,    11,\n",
      "          1415, 34196,   193,     4,     2]])\n",
      "tensor([[    0,    55,  8083,   208,   110,    47,  1362, 17260,  1294,  3280,\n",
      "           255,   994,    15,    11,   383,  3430,   364,    38,   512,    61,\n",
      "            11,  5628, 36791, 25313,     7,   237,    24,  1895,     7,    11,\n",
      "          1415, 34196,   193,     4,     2]], device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "13\n",
      "dict_keys(['The', 'bodies', 'of', 'the', 'soldiers', 'were', 'recovered', 'after', 'concerted', 'efforts', 'Avalanche', 'Rescue', 'Teams', '(', 'ART', ')', ',', 'which', 'is', 'equipped', 'to', 'work', 'in', 'inhospitable', 'terrain', 'and', 'weather', 'conditions', '.'])\n",
      "tensor([[    0,    47,  5644,    15,     6,  6819,   147, 14595,   177,     6,\n",
      "         28782,  1535,  5785,    15,     6, 50852, 14663, 15706,    57, 13927,\n",
      "            60,     7,   248,    17, 18510,     9,   157,    16,   520, 62262,\n",
      "           503, 37222,    13,  1155,  5430,     4,     2]])\n",
      "tensor([[    0,    47,  5644,    15,     6,  6819,   147, 14595,   177,     6,\n",
      "         28782,  1535,  5785,    15,     6, 50852, 14663, 15706,    57, 13927,\n",
      "            60,     7,   248,    17, 18510,     9,   157,    16,   520, 62262,\n",
      "           503, 37222,    13,  1155,  5430,     4,     2]], device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "13\n",
      "dict_keys(['&', 'gt', ';', '*', 'Arrangements', 'are', 'in', 'place', 'to', 'carry', 'the', 'mortal', 'remains', 'of', 'martyrs', 'their', 'native', 'places', 'immediately', 'after', 'weather', 'becomes', 'clear', ',', 'Defence', 'Spokesman', 'Colonel', 'Rajesh', 'Kalia', 'said', '.'])\n",
      "tensor([[    0,    55,  8083,   208,   110,  1517, 36647, 11363,    41,    16,\n",
      "           430,     9,  3071,     6, 27093,  5237,    15,     6, 33248,  7372,\n",
      "             9,   130,  7468,  2128,  3714,   177,  1155,  3655,  1673,     7,\n",
      "         20231, 55976,   171, 22514, 12824,  7440,   368, 13077,   193,     4,\n",
      "             2]])\n",
      "tensor([[    0,    55,  8083,   208,   110,  1517, 36647, 11363,    41,    16,\n",
      "           430,     9,  3071,     6, 27093,  5237,    15,     6, 33248,  7372,\n",
      "             9,   130,  7468,  2128,  3714,   177,  1155,  3655,  1673,     7,\n",
      "         20231, 55976,   171, 22514, 12824,  7440,   368, 13077,   193,     4,\n",
      "             2]], device='cuda:0')\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I']\n",
      "===============\n",
      "1864 1864\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "tokenized_sentences=[]\n",
    "count=0\n",
    "with torch.no_grad():\n",
    "    for test_record in testset:\n",
    "        # print(test_record)\n",
    "        # test_record=test_record.lower()\n",
    "        tokenized_input=tokenizer(test_record)\n",
    "        initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
    "        token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
    "        input_ids = initial_input_ids.to(device)\n",
    "        tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "        # output = model(input_ids)\n",
    "\n",
    "        output = alt_model(input_ids)\n",
    "        \n",
    "        \n",
    "        prediction = (torch.argmax(output.logits, axis=2))\n",
    "        prediction = prediction.cpu().numpy().reshape(-1)\n",
    "        prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
    "        prediction_labels=collate_token_labels(token_dict, prediction_labels[1:-1])\n",
    "        predictions.append(prediction_labels)\n",
    "        tokenized_sentences.append(token_dict.keys())\n",
    "        assert (len(prediction_labels)==len(token_dict.keys()))\n",
    "\n",
    "        if(count<=5):\n",
    "            print(len(output.hidden_states))\n",
    "            print(token_dict.keys())\n",
    "            print(initial_input_ids)\n",
    "            print(input_ids)\n",
    "            print(prediction_labels)\n",
    "            print('===============')\n",
    "        count+=1\n",
    "\n",
    "print(len(predictions),len(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1614158960521,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "pFo8FNMvDQmD",
    "outputId": "1a803aca-4b00-4f40-8171-a4a31e75fdbd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dict_keys(['&', 'gt', ';', '*', 'The', 'soldier', 'was', 'killed', 'when', 'another', 'avalanche', 'hit', 'an', 'army', 'barracks', 'in', 'the', 'northern', 'area', 'of', 'Sonmarg', ',', 'said', 'a', 'military', 'spokesman', '.']), dict_keys(['&', 'gt', ';', '*', 'Police', 'last', 'week', 'evacuated', '80', 'villagers', 'from', 'Waltengoo', 'Nar', 'where', 'dozens', 'were', 'killed', 'after', 'a', 'series', 'of', 'avalanches', 'hit', 'the', 'area', 'in', '2005', 'south', 'territory', '.']), dict_keys(['&', 'gt', ';', '*', 'The', 'army', 'on', 'Thursday', 'recovered', 'the', 'bodies', 'of', 'ten', 'its', 'men', 'who', 'were', 'killed', 'in', 'an', 'avalanche', 'previous', 'day', '.']), dict_keys(['&', 'gt', ';', '*', 'The', 'four', 'civilians', 'killed', 'included', 'two', 'children', 'of', 'a', 'family', 'whose', 'house', 'was', 'hit', 'by', 'separate', 'avalanche', ',', 'also', 'on', 'Wednesday', 'police', 'spokesman', 'said', '.']), dict_keys(['The', 'bodies', 'of', 'the', 'soldiers', 'were', 'recovered', 'after', 'concerted', 'efforts', 'Avalanche', 'Rescue', 'Teams', '(', 'ART', ')', ',', 'which', 'is', 'equipped', 'to', 'work', 'in', 'inhospitable', 'terrain', 'and', 'weather', 'conditions', '.'])]\n",
      "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentences[:5])\n",
    "print(predictions[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1157,
     "status": "ok",
     "timestamp": 1614158965139,
     "user": {
      "displayName": "Satadisha Saha Bhowmick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gjvn72nvq1ucp42Fo8VFDuvO6aS3LPxN5K-GEH_=s64",
      "userId": "12610541043224175650"
     },
     "user_tz": 300
    },
    "id": "-5RnF9Es20I1",
    "outputId": "f02d6f5f-d698-4461-a36e-d357869a5981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tally: 1864 1864\n",
      "0 ['sonmarg'] ['sonmarg']\n",
      "1 0 0\n",
      "1 ['waltengoo nar'] ['waltengoo', 'nar']\n",
      "0 2 1\n",
      "2 [] []\n",
      "0 0 0\n",
      "3 [] []\n",
      "0 0 0\n",
      "4 ['avalanche rescue teams', 'art'] []\n",
      "0 0 2\n",
      "5 ['colonel rajesh kalia'] ['kalia said']\n",
      "0 1 1\n",
      "6 ['gurez sector'] ['gurez']\n",
      "0 1 1\n",
      "7 ['ani', 'gurez sector'] ['ani', 'gurez']\n",
      "1 1 1\n",
      "8 ['ani'] []\n",
      "0 0 1\n",
      "9 ['rajesh kalia'] ['rajesh', 'kalia said']\n",
      "0 2 1\n",
      "10 ['what else is making news'] []\n",
      "0 0 1\n",
      "11 ['mahazgund'] ['mahazgund', 'ensured']\n",
      "1 1 0\n",
      "12 ['arts'] []\n",
      "0 0 1\n",
      "13 [] []\n",
      "0 0 0\n",
      "14 [] []\n",
      "0 0 0\n",
      "15 ['srinagar'] ['srinagar']\n",
      "1 0 0\n",
      "16 [] []\n",
      "0 0 0\n",
      "17 ['drdo'] []\n",
      "0 0 1\n",
      "18 ['drdo'] []\n",
      "0 0 1\n",
      "19 [] []\n",
      "0 0 0\n",
      "20 ['defence research development organisation', 'drdo'] ['drdo']\n",
      "1 0 1\n",
      "21 ['drdo'] []\n",
      "0 0 1\n",
      "22 ['bhamre', 'siachen glacier'] ['bhamre', 'siachen', 'glacier']\n",
      "1 2 1\n",
      "23 ['siachen'] ['siachen']\n",
      "1 0 0\n",
      "24 [] []\n",
      "0 0 0\n",
      "25 [] []\n",
      "0 0 0\n",
      "26 [] []\n",
      "0 0 0\n",
      "27 [] []\n",
      "0 0 0\n",
      "28 ['trump'] []\n",
      "0 0 1\n",
      "29 ['bhangra', 'punjabi'] []\n",
      "0 0 2\n",
      "30 [] []\n",
      "0 0 0\n",
      "31 ['toni kroos'] ['toni kroos']\n",
      "1 0 0\n",
      "32 ['chad'] ['chad']\n",
      "1 0 0\n",
      "33 [] []\n",
      "0 0 0\n",
      "34 [] []\n",
      "0 0 0\n",
      "35 [] []\n",
      "0 0 0\n",
      "36 [] []\n",
      "0 0 0\n",
      "37 ['keine bremsen'] ['keine bremsen']\n",
      "1 0 0\n",
      "38 ['jina'] ['jina']\n",
      "1 0 0\n",
      "39 ['europe'] ['europe']\n",
      "1 0 0\n",
      "40 ['bordeaux'] ['bordeaux']\n",
      "1 0 0\n",
      "41 [] []\n",
      "0 0 0\n",
      "42 [] []\n",
      "0 0 0\n",
      "43 [] []\n",
      "0 0 0\n",
      "44 ['trump'] ['has']\n",
      "0 1 1\n",
      "45 ['fragrant', 'sid'] ['fragrant', 'edge', 'sid']\n",
      "2 1 0\n",
      "46 [] []\n",
      "0 0 0\n",
      "47 [] []\n",
      "0 0 0\n",
      "48 [] []\n",
      "0 0 0\n",
      "49 [] []\n",
      "0 0 0\n",
      "50 ['glenn greenwald'] ['glenn greenwald']\n",
      "1 0 0\n",
      "51 [] []\n",
      "0 0 0\n",
      "52 [] []\n",
      "0 0 0\n",
      "53 ['democrats', 'trump'] ['elected']\n",
      "0 1 2\n",
      "54 ['living computer museum + labs', 'skyview observatory', 'smith tower', 'observation deck'] ['living computer museum + labs', 'skyview observatory', 'tower and', 'observation', 'deck']\n",
      "2 3 2\n",
      "55 ['td'] []\n",
      "0 0 1\n",
      "56 [] []\n",
      "0 0 0\n",
      "57 ['cvs', 'epipen'] ['cvs']\n",
      "1 0 1\n",
      "58 ['trump', 'michigan'] ['trump']\n",
      "1 0 1\n",
      "59 ['becky', 'snickers'] ['becky']\n",
      "1 0 1\n",
      "60 [] []\n",
      "0 0 0\n",
      "61 ['auckland'] ['auckland']\n",
      "1 0 0\n",
      "62 ['ireland'] ['ireland']\n",
      "1 0 0\n",
      "63 ['leyonhjelm'] ['leyonhjelm']\n",
      "1 0 0\n",
      "64 ['grimaldo', 'mu'] ['grimaldo', 'mu']\n",
      "2 0 0\n",
      "65 ['boro'] ['boro']\n",
      "1 0 0\n",
      "66 ['calgary'] ['calgary', 'too']\n",
      "1 1 0\n",
      "67 [] []\n",
      "0 0 0\n",
      "68 ['cavani', 'europe'] ['cavani']\n",
      "1 0 1\n",
      "69 ['cheney'] ['cheney']\n",
      "1 0 0\n",
      "70 ['trey gowdy', 'jason chaffetz'] ['trey gowdy', 'jason chaffetz']\n",
      "2 0 0\n",
      "71 [] []\n",
      "0 0 0\n",
      "72 ['dzeko'] ['dzeko']\n",
      "1 0 0\n",
      "73 ['r / news', 'r / politics'] []\n",
      "0 0 2\n",
      "74 ['lynda'] ['lynda']\n",
      "1 0 0\n",
      "75 ['calgary'] []\n",
      "0 0 1\n",
      "76 ['ronaldo'] ['criticism']\n",
      "0 1 1\n",
      "77 [] []\n",
      "0 0 0\n",
      "78 ['quickloot'] []\n",
      "0 0 1\n",
      "79 ['napoleon', 'ireland'] ['horse', 'to', 'ireland']\n",
      "1 2 1\n",
      "80 [] []\n",
      "0 0 0\n",
      "81 ['cvg'] []\n",
      "0 0 1\n",
      "82 [] ['reddit']\n",
      "0 1 0\n",
      "83 [] []\n",
      "0 0 0\n",
      "84 ['zlatan'] ['zlatan']\n",
      "1 0 0\n",
      "85 ['mods', 'trump'] ['trump']\n",
      "1 0 1\n",
      "86 ['costello'] ['sold']\n",
      "0 1 1\n",
      "87 ['google'] ['google']\n",
      "1 0 0\n",
      "88 [] []\n",
      "0 0 0\n",
      "89 [] []\n",
      "0 0 0\n",
      "90 [] []\n",
      "0 0 0\n",
      "91 [] []\n",
      "0 0 0\n",
      "92 ['bellerin', 'walker'] ['bellerin', 'walker']\n",
      "2 0 0\n",
      "93 ['barca'] []\n",
      "0 0 1\n",
      "94 ['syria', 'serbia'] ['serbia', 'guy']\n",
      "1 1 1\n",
      "95 ['tom'] ['tom']\n",
      "1 0 0\n",
      "96 [] []\n",
      "0 0 0\n",
      "97 ['andrew little'] ['little cheaper']\n",
      "0 1 1\n",
      "98 ['the x-files'] ['x-files']\n",
      "0 1 1\n",
      "99 ['psg', 'inter', 'juve', 'verratti'] ['verratti', 'will cost €']\n",
      "1 1 3\n",
      "100 ['donald'] ['donald']\n",
      "1 0 0\n",
      "101 ['trump'] ['continue']\n",
      "0 1 1\n",
      "102 [] []\n",
      "0 0 0\n",
      "103 ['png'] ['png']\n",
      "1 0 0\n",
      "104 ['great southern television'] ['great', 'southern television']\n",
      "0 2 1\n",
      "105 ['muslim'] []\n",
      "0 0 1\n",
      "106 ['huron'] ['huron']\n",
      "1 0 0\n",
      "107 ['scout'] ['scout']\n",
      "1 0 0\n",
      "108 ['morocco'] ['morocco']\n",
      "1 0 0\n",
      "109 ['costa rica'] ['costa', 'rica']\n",
      "0 2 1\n",
      "110 [] []\n",
      "0 0 0\n",
      "111 ['b . c'] []\n",
      "0 0 1\n",
      "112 ['r / southafrica'] []\n",
      "0 0 1\n",
      "113 ['ira'] []\n",
      "0 0 1\n",
      "114 ['aa'] []\n",
      "0 0 1\n",
      "115 [] []\n",
      "0 0 0\n",
      "116 ['paul manafort'] ['manafort did']\n",
      "0 1 1\n",
      "117 ['calgary'] ['calgary', 'which']\n",
      "1 1 0\n",
      "118 ['cbc manitoba'] ['cbc', 'manitoba']\n",
      "0 2 1\n",
      "119 ['diego rolan', 'psg'] ['goal vs psg']\n",
      "0 1 2\n",
      "120 [] []\n",
      "0 0 0\n",
      "121 ['rastus', 'pete'] ['rastus', 'pete']\n",
      "2 0 0\n",
      "122 [] []\n",
      "0 0 0\n",
      "123 [] []\n",
      "0 0 0\n",
      "124 [] []\n",
      "0 0 0\n",
      "125 ['concorde', 'calgary'] ['calgary', '1977']\n",
      "1 1 1\n",
      "126 [] []\n",
      "0 0 0\n",
      "127 [] []\n",
      "0 0 0\n",
      "128 [] []\n",
      "0 0 0\n",
      "129 [] []\n",
      "0 0 0\n",
      "130 [] []\n",
      "0 0 0\n",
      "131 ['adelaide', 'digeridoo'] ['adelaide']\n",
      "1 0 1\n",
      "132 ['ntep', 'costil', 'sagna'] ['ntep', 'costil', 'sagna would']\n",
      "2 1 1\n",
      "133 [] []\n",
      "0 0 0\n",
      "134 ['north korea'] ['north', 'korea']\n",
      "0 2 1\n",
      "135 [] []\n",
      "0 0 0\n",
      "136 ['peter hitchens'] ['peter hitchens']\n",
      "1 0 0\n",
      "137 ['republicans', 'bld', 'govt'] []\n",
      "0 0 3\n",
      "138 [\"stew ' s self service garage\"] ['stew']\n",
      "0 1 1\n",
      "139 ['snickers'] []\n",
      "0 0 1\n",
      "140 ['putin'] ['putin']\n",
      "1 0 0\n",
      "141 [] []\n",
      "0 0 0\n",
      "142 ['nhl'] []\n",
      "0 0 1\n",
      "143 [] []\n",
      "0 0 0\n",
      "144 ['uber'] ['uber']\n",
      "1 0 0\n",
      "145 [] []\n",
      "0 0 0\n",
      "146 [] []\n",
      "0 0 0\n",
      "147 [] []\n",
      "0 0 0\n",
      "148 ['rastus'] ['rastus']\n",
      "1 0 0\n",
      "149 [] []\n",
      "0 0 0\n",
      "150 ['labor'] ['labor']\n",
      "1 0 0\n",
      "151 [] []\n",
      "0 0 0\n",
      "152 ['buzzfeed'] []\n",
      "0 0 1\n",
      "153 ['canada', 'cbc', 'cons'] ['used', 'went']\n",
      "0 2 3\n",
      "154 ['marseille'] []\n",
      "0 0 1\n",
      "155 ['istanbul'] ['nightclub']\n",
      "0 1 1\n",
      "156 [] []\n",
      "0 0 0\n",
      "157 ['velvet'] ['velvet']\n",
      "1 0 0\n",
      "158 ['hector'] ['hector']\n",
      "1 0 0\n",
      "159 [] []\n",
      "0 0 0\n",
      "160 ['narsingh'] ['narsingh']\n",
      "1 0 0\n",
      "161 ['tomi lahren'] ['tomi lahren']\n",
      "1 0 0\n",
      "162 ['jamesakanoah'] []\n",
      "0 0 1\n",
      "163 [] []\n",
      "0 0 0\n",
      "164 [] []\n",
      "0 0 0\n",
      "165 ['ronaldo', 'madrid'] ['had']\n",
      "0 1 2\n",
      "166 [] []\n",
      "0 0 0\n",
      "167 ['stoke mafia'] ['stoke']\n",
      "0 1 1\n",
      "168 ['calgary'] ['calgary']\n",
      "1 0 0\n",
      "169 ['swansea'] ['swansea']\n",
      "1 0 0\n",
      "170 ['bex', 'snickers'] ['bex', 'snickers']\n",
      "2 0 0\n",
      "171 ['ireland'] ['be']\n",
      "0 1 1\n",
      "172 ['tom'] ['tom']\n",
      "1 0 0\n",
      "173 ['snickers'] []\n",
      "0 0 1\n",
      "174 ['andy carroll', 'crystal palace'] ['andy carroll', 'crystal palace']\n",
      "2 0 0\n",
      "175 [] []\n",
      "0 0 0\n",
      "176 [] []\n",
      "0 0 0\n",
      "177 ['ritchie mcclaw'] ['ritchie mcclaw']\n",
      "1 0 0\n",
      "178 [] []\n",
      "0 0 0\n",
      "179 [] []\n",
      "0 0 0\n",
      "180 ['trump'] ['trump']\n",
      "1 0 0\n",
      "181 ['u / nicklovettnz'] []\n",
      "0 0 1\n",
      "182 ['middleborough', 'jese rodriguez'] ['talks to']\n",
      "0 1 2\n",
      "183 [] []\n",
      "0 0 0\n",
      "184 ['barcelona', 'deulofeu', 'dongou', 'grimaldo'] ['barcelona', 'deulofeu', 'dongou and grimaldo']\n",
      "2 1 2\n",
      "185 ['jürgen klopp'] ['jürgen klopp']\n",
      "1 0 0\n",
      "186 ['multinational corporation'] ['multinational']\n",
      "0 1 1\n",
      "187 [] []\n",
      "0 0 0\n",
      "188 ['psg', 'gonçalo guedes'] ['gonçalo', 'guedes', 'for 25 million €']\n",
      "0 3 2\n",
      "189 [] []\n",
      "0 0 0\n",
      "190 [] ['shiba inu']\n",
      "0 1 0\n",
      "191 ['peter lim'] ['peter lim']\n",
      "1 0 0\n",
      "192 ['zm'] []\n",
      "0 0 1\n",
      "193 ['demolition man'] ['demolition man']\n",
      "1 0 0\n",
      "194 ['splc'] ['splc']\n",
      "1 0 0\n",
      "195 ['reddit'] []\n",
      "0 0 1\n",
      "196 ['fifa xi'] []\n",
      "0 0 1\n",
      "197 ['trump'] ['trump']\n",
      "1 0 0\n",
      "198 ['reddit'] ['reddit']\n",
      "1 0 0\n",
      "199 [] []\n",
      "0 0 0\n",
      "200 [] []\n",
      "0 0 0\n",
      "201 [] []\n",
      "0 0 0\n",
      "202 ['italy'] ['called']\n",
      "0 1 1\n",
      "203 [] []\n",
      "0 0 0\n",
      "204 ['local views'] []\n",
      "0 0 1\n",
      "205 [] []\n",
      "0 0 0\n",
      "206 ['mises'] ['mises']\n",
      "1 0 0\n",
      "207 [] []\n",
      "0 0 0\n",
      "208 ['maronti'] ['maronti']\n",
      "1 0 0\n",
      "209 [] []\n",
      "0 0 0\n",
      "210 [] []\n",
      "0 0 0\n",
      "211 ['aa'] []\n",
      "0 0 1\n",
      "212 ['aa'] []\n",
      "0 0 1\n",
      "213 [] []\n",
      "0 0 0\n",
      "214 ['ireland', 'iceland'] []\n",
      "0 0 2\n",
      "215 ['qatar 2022 world cup stadium'] ['cup']\n",
      "0 1 1\n",
      "216 ['trump', 'hillary'] ['trump', 'hillary']\n",
      "2 0 0\n",
      "217 [\"kelly ' s\", 'newtown'] ['kelly']\n",
      "0 1 2\n",
      "218 ['roz'] []\n",
      "0 0 1\n",
      "219 [] []\n",
      "0 0 0\n",
      "220 [] []\n",
      "0 0 0\n",
      "221 ['reddit'] []\n",
      "0 0 1\n",
      "222 ['reddit'] []\n",
      "0 0 1\n",
      "223 ['thorvirdh'] []\n",
      "0 0 1\n",
      "224 [] []\n",
      "0 0 0\n",
      "225 [] []\n",
      "0 0 0\n",
      "226 [] []\n",
      "0 0 0\n",
      "227 [] []\n",
      "0 0 0\n",
      "228 [] []\n",
      "0 0 0\n",
      "229 [] []\n",
      "0 0 0\n",
      "230 [] []\n",
      "0 0 0\n",
      "231 [] []\n",
      "0 0 0\n",
      "232 [] []\n",
      "0 0 0\n",
      "233 ['psg'] ['psg']\n",
      "1 0 0\n",
      "234 ['antónio guterres', 'un'] ['antónio guterres', 'un']\n",
      "2 0 0\n",
      "235 ['uber'] []\n",
      "0 0 1\n",
      "236 ['trump'] ['his']\n",
      "0 1 1\n",
      "237 ['sandringham'] ['sandringham']\n",
      "1 0 0\n",
      "238 [] []\n",
      "0 0 0\n",
      "239 [] []\n",
      "0 0 0\n",
      "240 [] []\n",
      "0 0 0\n",
      "241 [] []\n",
      "0 0 0\n",
      "242 [] []\n",
      "0 0 0\n",
      "243 ['conservative party'] ['party']\n",
      "0 1 1\n",
      "244 ['dalhousie'] ['dalhousie']\n",
      "1 0 0\n",
      "245 ['r / politics', 'r / politics'] []\n",
      "0 0 2\n",
      "246 [] []\n",
      "0 0 0\n",
      "247 [] []\n",
      "0 0 0\n",
      "248 ['r / politics', 'trump'] []\n",
      "0 0 2\n",
      "249 ['yaletown'] ['yaletown']\n",
      "1 0 0\n",
      "250 ['prem'] []\n",
      "0 0 1\n",
      "251 ['r / politics'] []\n",
      "0 0 1\n",
      "252 ['centrelink'] ['centrelink']\n",
      "1 0 0\n",
      "253 ['r', 'ireland'] []\n",
      "0 0 2\n",
      "254 ['r / southafrica'] ['southafrica']\n",
      "0 1 1\n",
      "255 ['bingo'] []\n",
      "0 0 1\n",
      "256 [] []\n",
      "0 0 0\n",
      "257 [] []\n",
      "0 0 0\n",
      "258 ['r / newzealand'] []\n",
      "0 0 1\n",
      "259 [] []\n",
      "0 0 0\n",
      "260 ['sudocrem'] []\n",
      "0 0 1\n",
      "261 [] []\n",
      "0 0 0\n",
      "262 ['womble _ don', 'lahm'] ['womble', 'don', 'lahm']\n",
      "1 2 1\n",
      "263 [] []\n",
      "0 0 0\n",
      "264 [] []\n",
      "0 0 0\n",
      "265 [] []\n",
      "0 0 0\n",
      "266 [] ['bordnamonalisa']\n",
      "0 1 0\n",
      "267 ['blm'] ['blm']\n",
      "1 0 0\n",
      "268 ['istanbul'] ['nightclub']\n",
      "0 1 1\n",
      "269 [] []\n",
      "0 0 0\n",
      "270 ['new zealand', 'reddit . com / r / newzealand'] ['zealand', 'could']\n",
      "0 2 2\n",
      "271 ['they', 'them'] []\n",
      "0 0 2\n",
      "272 ['internode'] []\n",
      "0 0 1\n",
      "273 ['the clippers'] ['clippers']\n",
      "0 1 1\n",
      "274 ['tottenham', 'chelsea', 'giroud'] ['tottenham', 'chelsea']\n",
      "2 0 1\n",
      "275 [] []\n",
      "0 0 0\n",
      "276 [] []\n",
      "0 0 0\n",
      "277 ['trump', 'nazis', 'trump'] ['trump']\n",
      "1 0 2\n",
      "278 ['wellington'] []\n",
      "0 0 1\n",
      "279 [] []\n",
      "0 0 0\n",
      "280 ['the twilight zone'] ['twilight zone']\n",
      "0 1 1\n",
      "281 [] []\n",
      "0 0 0\n",
      "282 ['bernie'] ['bernie']\n",
      "1 0 0\n",
      "283 [] []\n",
      "0 0 0\n",
      "284 [] []\n",
      "0 0 0\n",
      "285 ['ess jay dubs'] []\n",
      "0 0 1\n",
      "286 [] []\n",
      "0 0 0\n",
      "287 [] []\n",
      "0 0 0\n",
      "288 ['reddit'] []\n",
      "0 0 1\n",
      "289 ['jesus christ'] []\n",
      "0 0 1\n",
      "290 ['chelsea', 'r / soccer'] ['chelsea']\n",
      "1 0 1\n",
      "291 ['ken king', 'calgarynext'] ['ken king', 'calgarynext']\n",
      "2 0 0\n",
      "292 [] []\n",
      "0 0 0\n",
      "293 ['messi'] ['messi']\n",
      "1 0 0\n",
      "294 ['adebayor', 'basaksehir'] ['adebayor', 'signs for turkish', 'basaksehir']\n",
      "2 1 0\n",
      "295 [] []\n",
      "0 0 0\n",
      "296 [] []\n",
      "0 0 0\n",
      "297 ['democrats', 'nazi', 'cia'] []\n",
      "0 0 3\n",
      "298 [] []\n",
      "0 0 0\n",
      "299 ['virgin active', 'moore park', 'zetland'] ['at moore', 'zetland', 'for', 'you']\n",
      "1 3 2\n",
      "300 [] []\n",
      "0 0 0\n",
      "301 ['super bowl'] []\n",
      "0 0 1\n",
      "302 [] []\n",
      "0 0 0\n",
      "303 ['dark knight'] []\n",
      "0 0 1\n",
      "304 ['zlatan'] ['zlatan']\n",
      "1 0 0\n",
      "305 [] []\n",
      "0 0 0\n",
      "306 ['zealandia'] ['zealandia', 'before']\n",
      "1 1 0\n",
      "307 ['united'] []\n",
      "0 0 1\n",
      "308 ['trudeau'] ['trudeau']\n",
      "1 0 0\n",
      "309 ['sky sports'] ['sky sports']\n",
      "1 0 0\n",
      "310 ['jay 911'] []\n",
      "0 0 1\n",
      "311 ['montanskittensighs'] []\n",
      "0 0 1\n",
      "312 [] []\n",
      "0 0 0\n",
      "313 ['r / politics', 'trump'] ['supporters']\n",
      "0 1 2\n",
      "314 [] []\n",
      "0 0 0\n",
      "315 ['r', 'calgary'] ['r', 'calgary']\n",
      "2 0 0\n",
      "316 ['corbyn'] ['corbyn']\n",
      "1 0 0\n",
      "317 ['leave'] []\n",
      "0 0 1\n",
      "318 [] []\n",
      "0 0 0\n",
      "319 ['europe'] ['europe']\n",
      "1 0 0\n",
      "320 [] []\n",
      "0 0 0\n",
      "321 ['r / sydney'] []\n",
      "0 0 1\n",
      "322 [] []\n",
      "0 0 0\n",
      "323 [] []\n",
      "0 0 0\n",
      "324 ['coleman', 'aspects of symmetry'] ['coleman']\n",
      "1 0 1\n",
      "325 [] []\n",
      "0 0 0\n",
      "326 ['kirchhoff'] ['s law']\n",
      "0 1 1\n",
      "327 ['star trek'] ['star trek']\n",
      "1 0 0\n",
      "328 ['rogue one', 'imperial droids'] ['rogue one']\n",
      "1 0 1\n",
      "329 [] []\n",
      "0 0 0\n",
      "330 ['transcendent man'] []\n",
      "0 0 1\n",
      "331 [] []\n",
      "0 0 0\n",
      "332 [] []\n",
      "0 0 0\n",
      "333 [] []\n",
      "0 0 0\n",
      "334 ['homer', 'cuba'] ['is', 'onwards']\n",
      "0 2 2\n",
      "335 [] ['smedleydslap']\n",
      "0 1 0\n",
      "336 [] []\n",
      "0 0 0\n",
      "337 ['trump'] ['trump']\n",
      "1 0 0\n",
      "338 [] []\n",
      "0 0 0\n",
      "339 [] []\n",
      "0 0 0\n",
      "340 ['trump', 'snowden'] ['trump', 'snowden']\n",
      "2 0 0\n",
      "341 ['the day of the doctor'] []\n",
      "0 0 1\n",
      "342 [] []\n",
      "0 0 0\n",
      "343 [] []\n",
      "0 0 0\n",
      "344 [] []\n",
      "0 0 0\n",
      "345 [] []\n",
      "0 0 0\n",
      "346 [] []\n",
      "0 0 0\n",
      "347 ['clay'] ['clay']\n",
      "1 0 0\n",
      "348 [] []\n",
      "0 0 0\n",
      "349 ['olive', 'emma', \"miss peregrine ' s home for peculiar children\"] ['olive', 'emma', 'miss peregrine', 'for peculiar children']\n",
      "2 2 1\n",
      "350 [] []\n",
      "0 0 0\n",
      "351 ['lee'] ['daughter']\n",
      "0 1 1\n",
      "352 ['wifi'] []\n",
      "0 0 1\n",
      "353 [] []\n",
      "0 0 0\n",
      "354 ['senate democrats', 'republican', 'republicans', 'democrats', 'the republican administration nominees', 'democrats'] []\n",
      "0 0 6\n",
      "355 ['voyager'] ['voyager']\n",
      "1 0 0\n",
      "356 ['porygon', 'pokémon'] ['porygon', 'pokémon']\n",
      "2 0 0\n",
      "357 ['parliament', 'parliament'] []\n",
      "0 0 2\n",
      "358 ['openssh'] []\n",
      "0 0 1\n",
      "359 [] []\n",
      "0 0 0\n",
      "360 [] []\n",
      "0 0 0\n",
      "361 ['rick', 'dimension', 'dimensions'] ['rick']\n",
      "1 0 2\n",
      "362 ['ballistic galvanometer', 'classic galvanometer'] []\n",
      "0 0 2\n",
      "363 ['garvan', 'physics se'] ['garvan']\n",
      "1 0 1\n",
      "364 ['tor'] []\n",
      "0 0 1\n",
      "365 ['java object encryption'] []\n",
      "0 0 1\n",
      "366 ['eastern europe', 'orban', 'western european'] ['eastern', 'europe', 'orban']\n",
      "1 2 2\n",
      "367 ['amazon s 3'] []\n",
      "0 0 1\n",
      "368 [] []\n",
      "0 0 0\n",
      "369 ['java . security . publickey object'] []\n",
      "0 0 1\n",
      "370 ['supreme court'] []\n",
      "0 0 1\n",
      "371 ['outsiders'] []\n",
      "0 0 1\n",
      "372 ['ryll', 'kessel', 'anh'] ['ryll', 'kessel', 'anh']\n",
      "3 0 0\n",
      "373 ['xantec', 'hogwarts', 'diagon alley', 'dumbledore', 'hagrid', 'hagrid', 'harry', 'da'] ['hogwarts', 'diagon alley', 'dumbledore', 'hagrid', 'hagrid', 'harry']\n",
      "6 0 2\n",
      "374 [] []\n",
      "0 0 0\n",
      "375 ['aliens'] []\n",
      "0 0 1\n",
      "376 ['photons'] []\n",
      "0 0 1\n",
      "377 [] []\n",
      "0 0 0\n",
      "378 [] []\n",
      "0 0 0\n",
      "379 ['vrĳzinnige partĳ', 'groep klein', 'nieuwe wegen', 'monasch', 'vrĳzinnige partĳ', 'klein'] ['klein', 'nieuwe wegen', 'monasch']\n",
      "3 0 3\n",
      "380 [] []\n",
      "0 0 0\n",
      "381 [] []\n",
      "0 0 0\n",
      "382 [] []\n",
      "0 0 0\n",
      "383 ['t- 1000 terminator', 't- 800'] []\n",
      "0 0 2\n",
      "384 [] []\n",
      "0 0 0\n",
      "385 [] []\n",
      "0 0 0\n",
      "386 ['steeven'] ['steeven']\n",
      "1 0 0\n",
      "387 [] []\n",
      "0 0 0\n",
      "388 ['op'] []\n",
      "0 0 1\n",
      "389 ['cos', 'jo', 'order', 'hasbro chocolate frog cards', 'jo', 'lexicon'] ['http://www.jkrowling.com/textonly/en/faq_view.cfm?id=96', 'hasbro', 'did']\n",
      "0 3 6\n",
      "390 [] []\n",
      "0 0 0\n",
      "391 [] []\n",
      "0 0 0\n",
      "392 ['carpetsmoker', 'eastern europeans', 'western europe'] ['western', 'europe']\n",
      "0 2 3\n",
      "393 [] []\n",
      "0 0 0\n",
      "394 ['wildcard certificate private key security'] []\n",
      "0 0 1\n",
      "395 ['brythan'] []\n",
      "0 0 1\n",
      "396 ['qmechanic'] []\n",
      "0 0 1\n",
      "397 ['htaccess'] []\n",
      "0 0 1\n",
      "398 ['cphpython'] []\n",
      "0 0 1\n",
      "399 [] []\n",
      "0 0 0\n",
      "400 ['thorin', 'arkenstone'] ['thorin']\n",
      "1 0 1\n",
      "401 ['cryptonomicon'] []\n",
      "0 0 1\n",
      "402 ['trilarion'] []\n",
      "0 0 1\n",
      "403 ['law . se'] []\n",
      "0 0 1\n",
      "404 [] []\n",
      "0 0 0\n",
      "405 ['physics stack exchange'] []\n",
      "0 0 1\n",
      "406 ['professor monkey- for- a-head', 'earthworm jim'] ['professor monkey', 'earthworm jim']\n",
      "1 1 1\n",
      "407 [] []\n",
      "0 0 0\n",
      "408 ['h . r . 720'] []\n",
      "0 0 1\n",
      "409 ['hanzo', 'predators'] ['hanzo', 'predators']\n",
      "2 0 0\n",
      "410 [] []\n",
      "0 0 0\n",
      "411 ['fido u 2 f'] []\n",
      "0 0 1\n",
      "412 ['borg', 'delta quadrant'] []\n",
      "0 0 2\n",
      "413 ['amazon echo', 'dot'] ['echo', 'dot']\n",
      "1 1 1\n",
      "414 [] []\n",
      "0 0 0\n",
      "415 [] []\n",
      "0 0 0\n",
      "416 [] []\n",
      "0 0 0\n",
      "417 ['the six thatchers', 'watson', 'sherlock'] ['thatchers', 'blame', 'rather']\n",
      "0 3 3\n",
      "418 [] []\n",
      "0 0 0\n",
      "419 ['se'] []\n",
      "0 0 1\n",
      "420 ['tie fighter pilots'] []\n",
      "0 0 1\n",
      "421 [] []\n",
      "0 0 0\n",
      "422 ['q', 'déjà q', 'q'] []\n",
      "0 0 3\n",
      "423 [] []\n",
      "0 0 0\n",
      "424 ['xmen 3'] ['xmen']\n",
      "0 1 1\n",
      "425 [] []\n",
      "0 0 0\n",
      "426 [] []\n",
      "0 0 0\n",
      "427 ['mithril', 'mithril', 'the shire'] []\n",
      "0 0 3\n",
      "428 [] []\n",
      "0 0 0\n",
      "429 [] []\n",
      "0 0 0\n",
      "430 ['internet'] []\n",
      "0 0 1\n",
      "431 ['star trek movie', 'tos font'] ['star trek']\n",
      "0 1 2\n",
      "432 [] []\n",
      "0 0 0\n",
      "433 [] []\n",
      "0 0 0\n",
      "434 ['sql'] []\n",
      "0 0 1\n",
      "435 ['p 12 format', 'pem format', 'p 12', 'p 12 format', 'p 12'] []\n",
      "0 0 5\n",
      "436 [] []\n",
      "0 0 0\n",
      "437 [] []\n",
      "0 0 0\n",
      "438 ['tarzan', 'cheeta', 'cheeta'] ['tarzan', 'cheeta', 'a', 'good']\n",
      "2 2 1\n",
      "439 ['tobacco companies'] []\n",
      "0 0 1\n",
      "440 ['united states'] ['states', 'get']\n",
      "0 2 1\n",
      "441 [] []\n",
      "0 0 0\n",
      "442 ['president trump'] ['president trump']\n",
      "1 0 0\n",
      "443 [] []\n",
      "0 0 0\n",
      "444 [] []\n",
      "0 0 0\n",
      "445 [] []\n",
      "0 0 0\n",
      "446 ['tos', 'metamorphosis'] []\n",
      "0 0 2\n",
      "447 ['501 ( c ) 3', 'trump'] []\n",
      "0 0 2\n",
      "448 [] []\n",
      "0 0 0\n",
      "449 ['access control primitives'] []\n",
      "0 0 1\n",
      "450 [] []\n",
      "0 0 0\n",
      "451 [] []\n",
      "0 0 0\n",
      "452 [] []\n",
      "0 0 0\n",
      "453 ['touchscreen'] []\n",
      "0 0 1\n",
      "454 [] []\n",
      "0 0 0\n",
      "455 [] ['dirac hamiltonian']\n",
      "0 1 0\n",
      "456 ['agra drives'] []\n",
      "0 0 1\n",
      "457 [] []\n",
      "0 0 0\n",
      "458 [] []\n",
      "0 0 0\n",
      "459 ['digimon'] ['digimon']\n",
      "1 0 0\n",
      "460 ['vietnam'] ['and']\n",
      "0 1 1\n",
      "461 ['gmail'] []\n",
      "0 0 1\n",
      "462 [] []\n",
      "0 0 0\n",
      "463 ['the actual doctor'] []\n",
      "0 0 1\n",
      "464 [] []\n",
      "0 0 0\n",
      "465 ['paul'] ['paul']\n",
      "1 0 0\n",
      "466 [] []\n",
      "0 0 0\n",
      "467 ['poisson'] []\n",
      "0 0 1\n",
      "468 ['minas tirith', 'lúthien'] ['minas tirith', 'lúthien broken']\n",
      "1 1 1\n",
      "469 [] []\n",
      "0 0 0\n",
      "470 [] []\n",
      "0 0 0\n",
      "471 ['neville', 'carrow', 'neville', 'seamus'] ['neville', 'supporters', 'be', 'neville', 'seamus']\n",
      "3 2 1\n",
      "472 ['labour party'] []\n",
      "0 0 1\n",
      "473 [] []\n",
      "0 0 0\n",
      "474 ['tv tropes', 'rule of cool'] []\n",
      "0 0 2\n",
      "475 ['trump'] ['trump']\n",
      "1 0 0\n",
      "476 [] []\n",
      "0 0 0\n",
      "477 [] []\n",
      "0 0 0\n",
      "478 [] []\n",
      "0 0 0\n",
      "479 [] []\n",
      "0 0 0\n",
      "480 [] []\n",
      "0 0 0\n",
      "481 ['authy'] ['authy']\n",
      "1 0 0\n",
      "482 [] []\n",
      "0 0 0\n",
      "483 [] []\n",
      "0 0 0\n",
      "484 ['snape', 'voldemort'] ['snape', 'voldemort']\n",
      "2 0 0\n",
      "485 [] []\n",
      "0 0 0\n",
      "486 ['snowman'] []\n",
      "0 0 1\n",
      "487 ['windows event log'] []\n",
      "0 0 1\n",
      "488 ['the giants', 'earth'] []\n",
      "0 0 2\n",
      "489 [] []\n",
      "0 0 0\n",
      "490 ['king arthur', 'sword in stone', 'excalibur', 'ai robot scepter'] ['king arthur', 'sword in stone', 'excalibur']\n",
      "3 0 1\n",
      "491 ['star wars', 'carrie fisher'] ['wars series', 'fisher']\n",
      "0 2 2\n",
      "492 [] []\n",
      "0 0 0\n",
      "493 [] []\n",
      "0 0 0\n",
      "494 [] []\n",
      "0 0 0\n",
      "495 [] []\n",
      "0 0 0\n",
      "496 ['iot', 'mirai', 'iot'] ['mirai']\n",
      "1 0 2\n",
      "497 ['ubuntu'] []\n",
      "0 0 1\n",
      "498 [] []\n",
      "0 0 0\n",
      "499 ['lekku'] []\n",
      "0 0 1\n",
      "500 [] []\n",
      "0 0 0\n",
      "501 [] []\n",
      "0 0 0\n",
      "502 ['batman', 'joker'] ['used']\n",
      "0 1 2\n",
      "503 ['donald duck'] ['donald duck']\n",
      "1 0 0\n",
      "504 [] []\n",
      "0 0 0\n",
      "505 [] []\n",
      "0 0 0\n",
      "506 [] []\n",
      "0 0 0\n",
      "507 [] []\n",
      "0 0 0\n",
      "508 ['prepaid master card', 'master card'] []\n",
      "0 0 2\n",
      "509 [] []\n",
      "0 0 0\n",
      "510 [] []\n",
      "0 0 0\n",
      "511 ['skooba'] []\n",
      "0 0 1\n",
      "512 [] []\n",
      "0 0 0\n",
      "513 ['theguest'] []\n",
      "0 0 1\n",
      "514 ['kiwi'] []\n",
      "0 0 1\n",
      "515 [] []\n",
      "0 0 0\n",
      "516 ['mandalorian', 'mandalorians'] ['mandalorian', 'mandalorians']\n",
      "2 0 0\n",
      "517 [] []\n",
      "0 0 0\n",
      "518 [\"ghostsec ' s pentest labs\"] []\n",
      "0 0 1\n",
      "519 [] []\n",
      "0 0 0\n",
      "520 [] []\n",
      "0 0 0\n",
      "521 ['the matrix', 'the nebuchadnezzar', 'neo'] ['matrix', 'nebuchadnezzar', 'neo']\n",
      "1 2 2\n",
      "522 ['edlothiad', 'valorum'] ['edlothiad']\n",
      "1 0 1\n",
      "523 [] []\n",
      "0 0 0\n",
      "524 [] []\n",
      "0 0 0\n",
      "525 [] []\n",
      "0 0 0\n",
      "526 ['oblivion'] ['oblivion']\n",
      "1 0 0\n",
      "527 [] []\n",
      "0 0 0\n",
      "528 ['the sopranos'] ['the sopranos']\n",
      "1 0 0\n",
      "529 ['op', 'vpn server'] []\n",
      "0 0 2\n",
      "530 [] []\n",
      "0 0 0\n",
      "531 [] []\n",
      "0 0 0\n",
      "532 [] []\n",
      "0 0 0\n",
      "533 [] []\n",
      "0 0 0\n",
      "534 [] ['hulk']\n",
      "0 1 0\n",
      "535 [] []\n",
      "0 0 0\n",
      "536 ['vulcan'] []\n",
      "0 0 1\n",
      "537 ['donnager'] ['donnager']\n",
      "1 0 0\n",
      "538 [] []\n",
      "0 0 0\n",
      "539 [] []\n",
      "0 0 0\n",
      "540 [] []\n",
      "0 0 0\n",
      "541 [] []\n",
      "0 0 0\n",
      "542 ['valorum'] ['valorum']\n",
      "1 0 0\n",
      "543 ['potterwatch'] ['potterwatch']\n",
      "1 0 0\n",
      "544 [] []\n",
      "0 0 0\n",
      "545 ['hogwarts', 'harry'] ['hogwarts', 'harry']\n",
      "2 0 0\n",
      "546 [] []\n",
      "0 0 0\n",
      "547 [] []\n",
      "0 0 0\n",
      "548 [] []\n",
      "0 0 0\n",
      "549 ['grandfather bank'] []\n",
      "0 0 1\n",
      "550 ['mathjax', 'mathjax', 'mathjax basic tutorial and quick reference'] []\n",
      "0 0 3\n",
      "551 [] []\n",
      "0 0 0\n",
      "552 ['negan'] ['wander', 'a']\n",
      "0 2 1\n",
      "553 [] []\n",
      "0 0 0\n",
      "554 [] []\n",
      "0 0 0\n",
      "555 [] []\n",
      "0 0 0\n",
      "556 ['apache 2 server'] []\n",
      "0 0 1\n",
      "557 ['the affair season 3 episode 10 noah half'] ['noah']\n",
      "0 1 1\n",
      "558 ['daario', 'euron'] ['i', 'euron']\n",
      "1 1 1\n",
      "559 ['supreme court judge'] []\n",
      "0 0 1\n",
      "560 ['dmckee'] []\n",
      "0 0 1\n",
      "561 ['rots', 'obi- wan', 'anakin', 'droids'] ['rots', 'obi- wan', 'anakin']\n",
      "3 0 1\n",
      "562 ['turkish military'] []\n",
      "0 0 1\n",
      "563 [] []\n",
      "0 0 0\n",
      "564 ['sql'] []\n",
      "0 0 1\n",
      "565 [] []\n",
      "0 0 0\n",
      "566 [] []\n",
      "0 0 0\n",
      "567 ['bebs'] []\n",
      "0 0 1\n",
      "568 [] []\n",
      "0 0 0\n",
      "569 ['tor hidden services'] []\n",
      "0 0 1\n",
      "570 [] []\n",
      "0 0 0\n",
      "571 [] []\n",
      "0 0 0\n",
      "572 [] []\n",
      "0 0 0\n",
      "573 [] []\n",
      "0 0 0\n",
      "574 [] []\n",
      "0 0 0\n",
      "575 [] []\n",
      "0 0 0\n",
      "576 [] []\n",
      "0 0 0\n",
      "577 [] []\n",
      "0 0 0\n",
      "578 ['thomas jane', 'the expanse'] ['thomas jane', 'expanse']\n",
      "1 1 1\n",
      "579 ['collide'] []\n",
      "0 0 1\n",
      "580 [] []\n",
      "0 0 0\n",
      "581 [] []\n",
      "0 0 0\n",
      "582 ['phoog', 'parliament'] []\n",
      "0 0 2\n",
      "583 [] []\n",
      "0 0 0\n",
      "584 [] []\n",
      "0 0 0\n",
      "585 ['beast', 'julia'] ['beast', 'julia']\n",
      "2 0 0\n",
      "586 [] []\n",
      "0 0 0\n",
      "587 ['french'] []\n",
      "0 0 1\n",
      "588 [] []\n",
      "0 0 0\n",
      "589 ['zaphod beeblebrox'] ['zaphod beeblebrox']\n",
      "1 0 0\n",
      "590 ['isabella', 'her'] []\n",
      "0 0 2\n",
      "591 [] []\n",
      "0 0 0\n",
      "592 ['mon- el'] ['mon', 'el']\n",
      "0 2 1\n",
      "593 ['movies & tv'] []\n",
      "0 0 1\n",
      "594 [] []\n",
      "0 0 0\n",
      "595 ['middle-earth'] ['middle-earth']\n",
      "1 0 0\n",
      "596 [] []\n",
      "0 0 0\n",
      "597 ['demi', 'python'] ['demi']\n",
      "1 0 1\n",
      "598 ['nl'] ['nl']\n",
      "1 0 0\n",
      "599 ['gordon'] ['gordon', 'higgs']\n",
      "1 1 0\n",
      "600 ['cadillac'] []\n",
      "0 0 1\n",
      "601 ['third hallow', 'cloak of invisibility', 'miss granger'] ['granger', 'demiguise']\n",
      "0 2 3\n",
      "602 ['sso'] []\n",
      "0 0 1\n",
      "603 [] []\n",
      "0 0 0\n",
      "604 ['mjölnirs'] []\n",
      "0 0 1\n",
      "605 [] []\n",
      "0 0 0\n",
      "606 ['clinton'] ['clinton']\n",
      "1 0 0\n",
      "607 [] []\n",
      "0 0 0\n",
      "608 ['trump', 'realdonaldtrump'] ['trump', 'realdonaldtrump']\n",
      "2 0 0\n",
      "609 ['donatelloswansino'] []\n",
      "0 0 1\n",
      "610 ['star trek', 'star trek', 'kirk', 'making of star trek 1968 edition', 'the writers guide'] ['is a', 'trek 1968']\n",
      "0 2 5\n",
      "611 ['windows hello', 'windows 10'] []\n",
      "0 0 2\n",
      "612 [] []\n",
      "0 0 0\n",
      "613 ['martians'] []\n",
      "0 0 1\n",
      "614 ['democrats'] []\n",
      "0 0 1\n",
      "615 ['lelouch', 'universe'] ['lelouch']\n",
      "1 0 1\n",
      "616 ['senate', 'jedi council'] []\n",
      "0 0 2\n",
      "617 [] []\n",
      "0 0 0\n",
      "618 ['black swan'] ['black swan']\n",
      "1 0 0\n",
      "619 [] []\n",
      "0 0 0\n",
      "620 ['non- citizens'] []\n",
      "0 0 1\n",
      "621 [] []\n",
      "0 0 0\n",
      "622 ['wifi', 'wifi network', 'wifi chip', 'sd card'] []\n",
      "0 0 4\n",
      "623 ['oauth'] []\n",
      "0 0 1\n",
      "624 ['country', 'un', 'world bank'] []\n",
      "0 0 3\n",
      "625 ['javier'] ['javier']\n",
      "1 0 0\n",
      "626 ['assassins creed'] ['assassins creed']\n",
      "1 0 0\n",
      "627 [] []\n",
      "0 0 0\n",
      "628 [] []\n",
      "0 0 0\n",
      "629 [] []\n",
      "0 0 0\n",
      "630 [] []\n",
      "0 0 0\n",
      "631 ['planet'] []\n",
      "0 0 1\n",
      "632 [] []\n",
      "0 0 0\n",
      "633 [] []\n",
      "0 0 0\n",
      "634 [] []\n",
      "0 0 0\n",
      "635 ['xiongchiamiov'] []\n",
      "0 0 1\n",
      "636 ['the next three days'] []\n",
      "0 0 1\n",
      "637 [] []\n",
      "0 0 0\n",
      "638 [] []\n",
      "0 0 0\n",
      "639 [] []\n",
      "0 0 0\n",
      "640 ['yautja', 'super yautja', 'predators'] []\n",
      "0 0 3\n",
      "641 ['star trek tos', 'discovery'] ['star trek tos', 'discovery']\n",
      "2 0 0\n",
      "642 [] []\n",
      "0 0 0\n",
      "643 ['parts'] []\n",
      "0 0 1\n",
      "644 [] []\n",
      "0 0 0\n",
      "645 [] []\n",
      "0 0 0\n",
      "646 ['rival cannibal car gangs'] []\n",
      "0 0 1\n",
      "647 ['hogwarts', 'other houses'] ['hogwarts']\n",
      "1 0 1\n",
      "648 [] []\n",
      "0 0 0\n",
      "649 [] []\n",
      "0 0 0\n",
      "650 [] []\n",
      "0 0 0\n",
      "651 [] []\n",
      "0 0 0\n",
      "652 ['rogue one', 'return of the jedi'] ['rogue one', 'return of the jedi', 'hyperspace radio']\n",
      "2 1 0\n",
      "653 [] []\n",
      "0 0 0\n",
      "654 [] []\n",
      "0 0 0\n",
      "655 [] []\n",
      "0 0 0\n",
      "656 ['schroeder'] ['schroeder', 'minkowski', 'wick']\n",
      "1 2 0\n",
      "657 [] ['gallifreyan']\n",
      "0 1 0\n",
      "658 [] []\n",
      "0 0 0\n",
      "659 [] []\n",
      "0 0 0\n",
      "660 [\"asimov ' s\"] ['asimov']\n",
      "0 1 1\n",
      "661 ['keepass cracker'] []\n",
      "0 0 1\n",
      "662 [] []\n",
      "0 0 0\n",
      "663 [] []\n",
      "0 0 0\n",
      "664 [] []\n",
      "0 0 0\n",
      "665 ['jedi', 'dark side'] []\n",
      "0 0 2\n",
      "666 ['the military'] []\n",
      "0 0 1\n",
      "667 [] []\n",
      "0 0 0\n",
      "668 [] []\n",
      "0 0 0\n",
      "669 ['the administration'] []\n",
      "0 0 1\n",
      "670 ['town'] []\n",
      "0 0 1\n",
      "671 [] []\n",
      "0 0 0\n",
      "672 [] []\n",
      "0 0 0\n",
      "673 ['doctor who'] ['doctor who']\n",
      "1 0 0\n",
      "674 [] []\n",
      "0 0 0\n",
      "675 [] []\n",
      "0 0 0\n",
      "676 [] []\n",
      "0 0 0\n",
      "677 ['bernoulli'] ['bernoulli']\n",
      "1 0 0\n",
      "678 ['wookipedia', 'legends'] ['wookipedia']\n",
      "1 0 1\n",
      "679 ['george smiley', 'ann', 'bill haydon'] ['george smiley', 'ann', 'bill haydon']\n",
      "3 0 0\n",
      "680 ['european union', 'general data protection regulation'] ['european']\n",
      "0 1 2\n",
      "681 ['vinny'] ['shooting']\n",
      "0 1 1\n",
      "682 ['wikipedia', 'wikipedia'] []\n",
      "0 0 2\n",
      "683 [] []\n",
      "0 0 0\n",
      "684 [] []\n",
      "0 0 0\n",
      "685 ['klingons', 'federation'] []\n",
      "0 0 2\n",
      "686 [] []\n",
      "0 0 0\n",
      "687 ['22 nd amendment'] []\n",
      "0 0 1\n",
      "688 [] []\n",
      "0 0 0\n",
      "689 [] ['foucault pendulum']\n",
      "0 1 0\n",
      "690 [] []\n",
      "0 0 0\n",
      "691 [] []\n",
      "0 0 0\n",
      "692 [] []\n",
      "0 0 0\n",
      "693 [] []\n",
      "0 0 0\n",
      "694 ['abrikosov'] ['abrikosov']\n",
      "1 0 0\n",
      "695 [] []\n",
      "0 0 0\n",
      "696 [] []\n",
      "0 0 0\n",
      "697 ['powerless'] []\n",
      "0 0 1\n",
      "698 ['lily', 'james potter', 'hogwarts'] ['lily', 'james potter', 'hogwarts']\n",
      "3 0 0\n",
      "699 [] []\n",
      "0 0 0\n",
      "700 [] []\n",
      "0 0 0\n",
      "701 [] []\n",
      "0 0 0\n",
      "702 [] []\n",
      "0 0 0\n",
      "703 [] []\n",
      "0 0 0\n",
      "704 [] []\n",
      "0 0 0\n",
      "705 [] []\n",
      "0 0 0\n",
      "706 ['yakuza 0'] ['yakuza']\n",
      "0 1 1\n",
      "707 ['woods', 'kalupey'] ['kalupey']\n",
      "1 0 1\n",
      "708 [] []\n",
      "0 0 0\n",
      "709 [] []\n",
      "0 0 0\n",
      "710 [] []\n",
      "0 0 0\n",
      "711 [] []\n",
      "0 0 0\n",
      "712 [] []\n",
      "0 0 0\n",
      "713 [] []\n",
      "0 0 0\n",
      "714 [] []\n",
      "0 0 0\n",
      "715 [] []\n",
      "0 0 0\n",
      "716 [] []\n",
      "0 0 0\n",
      "717 ['trump'] ['trump']\n",
      "1 0 0\n",
      "718 [\"children ' s of a is\", 'birmingham'] ['children', 'a', 'birmingham']\n",
      "1 2 1\n",
      "719 ['iclever'] []\n",
      "0 0 1\n",
      "720 ['china'] []\n",
      "0 0 1\n",
      "721 ['burns', 'bobby burns'] ['bobby cocktail']\n",
      "0 1 2\n",
      "722 [] []\n",
      "0 0 0\n",
      "723 [] []\n",
      "0 0 0\n",
      "724 [] []\n",
      "0 0 0\n",
      "725 ['los angeles'] ['los', 'angeles']\n",
      "0 2 1\n",
      "726 ['omb', 'mick mulvaney'] ['mick mulvaney', 'social', 'medicare']\n",
      "1 2 1\n",
      "727 [] []\n",
      "0 0 0\n",
      "728 [] []\n",
      "0 0 0\n",
      "729 [] ['gwynstone']\n",
      "0 1 0\n",
      "730 [] []\n",
      "0 0 0\n",
      "731 ['kitt', 'kitt'] ['kitt', 'kitt']\n",
      "2 0 0\n",
      "732 ['rachel', 'audrey', 'audrey', 'rachel', 'audrey'] ['rachel', 'audrey', 'rachel', 'audrey']\n",
      "4 0 1\n",
      "733 [] []\n",
      "0 0 0\n",
      "734 ['jellombooty'] []\n",
      "0 0 1\n",
      "735 ['carolsankar'] ['carolsankar']\n",
      "1 0 0\n",
      "736 [] []\n",
      "0 0 0\n",
      "737 [] []\n",
      "0 0 0\n",
      "738 [] []\n",
      "0 0 0\n",
      "739 [] []\n",
      "0 0 0\n",
      "740 ['prompto', 'roen belt'] ['prompto', 'roen']\n",
      "1 1 1\n",
      "741 [] []\n",
      "0 0 0\n",
      "742 [] []\n",
      "0 0 0\n",
      "743 ['daily audio bible program'] []\n",
      "0 0 1\n",
      "744 ['tinder'] []\n",
      "0 0 1\n",
      "745 ['devonport'] ['devonport']\n",
      "1 0 0\n",
      "746 ['clarke'] ['as']\n",
      "0 1 1\n",
      "747 [\"kelli ' s cardio kickboxing workout - max calorie burn workout with no equipment\", 'fitnessblender'] ['kelli']\n",
      "0 1 2\n",
      "748 [] []\n",
      "0 0 0\n",
      "749 [] []\n",
      "0 0 0\n",
      "750 [] []\n",
      "0 0 0\n",
      "751 [] []\n",
      "0 0 0\n",
      "752 ['angryblackhoemo'] []\n",
      "0 0 1\n",
      "753 [] []\n",
      "0 0 0\n",
      "754 ['bots _ alive kit'] []\n",
      "0 0 1\n",
      "755 ['missmccleary'] ['missmccleary']\n",
      "1 0 0\n",
      "756 ['corvette', '2017 chevrolet corvette', 'grand sport 3 lt'] ['msrp']\n",
      "0 1 3\n",
      "757 ['nicholaspegg', 'the doctor'] ['nicholaspegg', 'doctor']\n",
      "1 1 1\n",
      "758 ['minnesota'] ['minnesota']\n",
      "1 0 0\n",
      "759 ['malachai parker', 'bonnie bennett'] ['malachai parker', 'bonnie bennett']\n",
      "2 0 0\n",
      "760 [] []\n",
      "0 0 0\n",
      "761 ['bap', 'bap'] ['bap', 'this']\n",
      "1 1 1\n",
      "762 [] []\n",
      "0 0 0\n",
      "763 ['rt'] []\n",
      "0 0 1\n",
      "764 [] []\n",
      "0 0 0\n",
      "765 ['finbergin'] []\n",
      "0 0 1\n",
      "766 ['timkaine'] ['timkaine']\n",
      "1 0 0\n",
      "767 [] []\n",
      "0 0 0\n",
      "768 [] []\n",
      "0 0 0\n",
      "769 ['margarita madness'] []\n",
      "0 0 1\n",
      "770 ['parkscience'] []\n",
      "0 0 1\n",
      "771 [] []\n",
      "0 0 0\n",
      "772 [] []\n",
      "0 0 0\n",
      "773 [] []\n",
      "0 0 0\n",
      "774 [] []\n",
      "0 0 0\n",
      "775 [] []\n",
      "0 0 0\n",
      "776 [] []\n",
      "0 0 0\n",
      "777 ['toronto'] ['toronto']\n",
      "1 0 0\n",
      "778 ['trump'] ['trump']\n",
      "1 0 0\n",
      "779 [] ['jay']\n",
      "0 1 0\n",
      "780 [] []\n",
      "0 0 0\n",
      "781 [] []\n",
      "0 0 0\n",
      "782 ['harry _ styles', 'glasgow'] ['harry _ styles', 'glasgow']\n",
      "2 0 0\n",
      "783 ['google', 'google'] ['google']\n",
      "1 0 1\n",
      "784 ['nazis'] []\n",
      "0 0 1\n",
      "785 [] []\n",
      "0 0 0\n",
      "786 [] []\n",
      "0 0 0\n",
      "787 [] []\n",
      "0 0 0\n",
      "788 ['bangkok', 'thailand'] ['bangkok', 'thailand']\n",
      "2 0 0\n",
      "789 ['lockheed', 'trump', 'f- 35'] ['lockheed']\n",
      "1 0 2\n",
      "790 ['brianklaas', 'badlands national park', 'trump'] ['badlands', 'national park', 'trump']\n",
      "1 2 2\n",
      "791 ['kembar mas utara', 'buahbatu'] []\n",
      "0 0 2\n",
      "792 [] []\n",
      "0 0 0\n",
      "793 [] []\n",
      "0 0 0\n",
      "794 [] []\n",
      "0 0 0\n",
      "795 [] []\n",
      "0 0 0\n",
      "796 [] []\n",
      "0 0 0\n",
      "797 [] []\n",
      "0 0 0\n",
      "798 [] []\n",
      "0 0 0\n",
      "799 ['the fortezza', 'florence'] ['fortezza', 'florence']\n",
      "1 1 1\n",
      "800 [] []\n",
      "0 0 0\n",
      "801 ['the knifing tourney 2017', 'judges'] []\n",
      "0 0 2\n",
      "802 ['thevidspot'] []\n",
      "0 0 1\n",
      "803 ['jojo'] []\n",
      "0 0 1\n",
      "804 ['graysondolan'] ['graysondolan']\n",
      "1 0 0\n",
      "805 [] ['dumptrump']\n",
      "0 1 0\n",
      "806 ['brandy'] ['brandy']\n",
      "1 0 0\n",
      "807 ['jfradioshow', 'senate', 'house', 'senate'] []\n",
      "0 0 4\n",
      "808 [] []\n",
      "0 0 0\n",
      "809 ['nike air jordan 11 xi'] []\n",
      "0 0 1\n",
      "810 ['edsheeran'] ['igbtryden', 'edsheeran', 'my']\n",
      "1 2 0\n",
      "811 [] []\n",
      "0 0 0\n",
      "812 [] ['jennajackson']\n",
      "0 1 0\n",
      "813 [] []\n",
      "0 0 0\n",
      "814 ['sacha baron cohen', 'the brothers', 'sachabaroncohen', 'sachabaroncohen'] ['sacha baron cohen', 'the brothers', 'sachabaroncohen']\n",
      "3 0 1\n",
      "815 [] []\n",
      "0 0 0\n",
      "816 [] []\n",
      "0 0 0\n",
      "817 [] []\n",
      "0 0 0\n",
      "818 ['barbie', 'ken'] ['and', 'don']\n",
      "0 2 2\n",
      "819 [] []\n",
      "0 0 0\n",
      "820 ['teddys', 'irinagomez'] []\n",
      "0 0 2\n",
      "821 ['nicole', 'raspberry sorbet gum'] ['nicole']\n",
      "1 0 1\n",
      "822 [] []\n",
      "0 0 0\n",
      "823 [] []\n",
      "0 0 0\n",
      "824 [] []\n",
      "0 0 0\n",
      "825 ['river rom', 'river beam', 'essex'] ['tributary']\n",
      "0 1 3\n",
      "826 [] []\n",
      "0 0 0\n",
      "827 [] []\n",
      "0 0 0\n",
      "828 [] []\n",
      "0 0 0\n",
      "829 [] []\n",
      "0 0 0\n",
      "830 [\"arri ' s playtime tv : how to start the fisher- p\"] ['arri', 'p']\n",
      "0 2 1\n",
      "831 [] []\n",
      "0 0 0\n",
      "832 [] []\n",
      "0 0 0\n",
      "833 ['astana', 'syria'] ['astana']\n",
      "1 0 1\n",
      "834 [] []\n",
      "0 0 0\n",
      "835 [] []\n",
      "0 0 0\n",
      "836 ['mcgregor', 'ewan', 'good morning britain', 'piersmorgan'] ['mcgregor', 'ewan', 'good morning britain', 'piersmorgan was']\n",
      "3 1 1\n",
      "837 [] []\n",
      "0 0 0\n",
      "838 ['darius'] ['darius']\n",
      "1 0 0\n",
      "839 ['the sheep'] []\n",
      "0 0 1\n",
      "840 ['obi wan kenobi', 'darth'] ['obi wan kenobi', 'darth', 'hamillhimself']\n",
      "2 1 0\n",
      "841 [] []\n",
      "0 0 0\n",
      "842 [] []\n",
      "0 0 0\n",
      "843 ['theresistance'] []\n",
      "0 0 1\n",
      "844 [] []\n",
      "0 0 0\n",
      "845 [] []\n",
      "0 0 0\n",
      "846 ['la la land', 'cnn'] ['la land ”']\n",
      "0 1 2\n",
      "847 [] []\n",
      "0 0 0\n",
      "848 [] []\n",
      "0 0 0\n",
      "849 [] ['savmontano']\n",
      "0 1 0\n",
      "850 [] []\n",
      "0 0 0\n",
      "851 [] []\n",
      "0 0 0\n",
      "852 ['in my bed', 'amy winehouse'] ['amy winehouse']\n",
      "1 0 1\n",
      "853 [] []\n",
      "0 0 0\n",
      "854 ['blog technology'] []\n",
      "0 0 1\n",
      "855 ['missuniverse', 'indonesia'] []\n",
      "0 0 2\n",
      "856 ['lawrence', 'nytimes'] ['lawrence']\n",
      "1 0 1\n",
      "857 ['trump', 'trump', 'mexico'] ['trump', 'and', 'mexico']\n",
      "2 1 1\n",
      "858 ['illuminatiam : the first testament of the illuminati'] []\n",
      "0 0 1\n",
      "859 ['trump', 'islamic terrorists'] ['trump']\n",
      "1 0 1\n",
      "860 [] []\n",
      "0 0 0\n",
      "861 [] []\n",
      "0 0 0\n",
      "862 [] []\n",
      "0 0 0\n",
      "863 [] []\n",
      "0 0 0\n",
      "864 [] []\n",
      "0 0 0\n",
      "865 [] []\n",
      "0 0 0\n",
      "866 [] []\n",
      "0 0 0\n",
      "867 [] []\n",
      "0 0 0\n",
      "868 [] []\n",
      "0 0 0\n",
      "869 ['the angels are listening', 'snatam kaur', 'suṉi-ai', 'ajeet kaur'] ['snatam kaur', 'ajeet at']\n",
      "1 1 3\n",
      "870 [] []\n",
      "0 0 0\n",
      "871 [] []\n",
      "0 0 0\n",
      "872 ['zoe _ clark'] ['zoe', 'clark']\n",
      "0 2 1\n",
      "873 [] []\n",
      "0 0 0\n",
      "874 [] []\n",
      "0 0 0\n",
      "875 [] []\n",
      "0 0 0\n",
      "876 ['jaketapper', 'white house'] []\n",
      "0 0 2\n",
      "877 ['leaves'] []\n",
      "0 0 1\n",
      "878 [] []\n",
      "0 0 0\n",
      "879 ['jiovannivargas'] []\n",
      "0 0 1\n",
      "880 ['yo soy moana ( canto ancestral', 'smule'] ['moana']\n",
      "0 1 2\n",
      "881 ['kezia warouw', 'missuniverse', 'indonesia'] ['kezia warouw']\n",
      "1 0 2\n",
      "882 [] []\n",
      "0 0 0\n",
      "883 ['nathanzed'] ['nathanzed']\n",
      "1 0 0\n",
      "884 [] []\n",
      "0 0 0\n",
      "885 [] []\n",
      "0 0 0\n",
      "886 ['seoul- incheon', 'south korea'] ['seoul', 'incheon', 'south', 'korea']\n",
      "0 4 2\n",
      "887 ['martinbelam'] ['martinbelam']\n",
      "1 0 0\n",
      "888 ['seanhannity', 'realdonaldtrump'] ['ingrahamangle', 'staff']\n",
      "0 2 2\n",
      "889 ['marykaycabot', 'thekennyroda', 'clevelanddotcom'] []\n",
      "0 0 3\n",
      "890 [] []\n",
      "0 0 0\n",
      "891 [] []\n",
      "0 0 0\n",
      "892 ['instagram'] []\n",
      "0 0 1\n",
      "893 ['lub'] []\n",
      "0 0 1\n",
      "894 ['tamara lovett'] ['tamara lovett']\n",
      "1 0 0\n",
      "895 [] []\n",
      "0 0 0\n",
      "896 [] []\n",
      "0 0 0\n",
      "897 ['gold rush', 'the pregame light show'] []\n",
      "0 0 2\n",
      "898 [] ['lance 210']\n",
      "0 1 0\n",
      "899 [] []\n",
      "0 0 0\n",
      "900 [] []\n",
      "0 0 0\n",
      "901 ['lyft'] []\n",
      "0 0 1\n",
      "902 ['sallykohn'] ['sallykohn']\n",
      "1 0 0\n",
      "903 ['elizabeth _ macg'] ['elizabeth']\n",
      "0 1 1\n",
      "904 ['badlandsnps'] []\n",
      "0 0 1\n",
      "905 ['ballhard', 'korin'] []\n",
      "0 0 2\n",
      "906 ['ti'] ['the']\n",
      "0 1 1\n",
      "907 [] []\n",
      "0 0 0\n",
      "908 [] []\n",
      "0 0 0\n",
      "909 [] []\n",
      "0 0 0\n",
      "910 ['maikokitamura', 'foolish love'] ['maikokitamura']\n",
      "1 0 1\n",
      "911 ['pt citilink indonesia', 'citilink'] []\n",
      "0 0 2\n",
      "912 [] []\n",
      "0 0 0\n",
      "913 [] []\n",
      "0 0 0\n",
      "914 ['nataliegborden', 'bannon'] ['nataliegborden', 'that is', 's']\n",
      "1 2 1\n",
      "915 [] []\n",
      "0 0 0\n",
      "916 [] []\n",
      "0 0 0\n",
      "917 [] []\n",
      "0 0 0\n",
      "918 [] []\n",
      "0 0 0\n",
      "919 ['twoblindbros', 'nbcnightlynews'] []\n",
      "0 0 2\n",
      "920 ['karibrownnn'] []\n",
      "0 0 1\n",
      "921 [] []\n",
      "0 0 0\n",
      "922 ['ney', 'marshals'] ['ney', 'had']\n",
      "1 1 1\n",
      "923 ['dell inspiron 17 r 5721 core i 5 - 3317 u 1 . 7 ghz 6 gb 750 gb dvdrw 17 . 3 \" laptop'] []\n",
      "0 0 1\n",
      "924 [] []\n",
      "0 0 0\n",
      "925 [] []\n",
      "0 0 0\n",
      "926 ['prisonplanet', 'kellyanne'] ['kellyanne']\n",
      "1 0 1\n",
      "927 ['ziploc'] []\n",
      "0 0 1\n",
      "928 [] []\n",
      "0 0 0\n",
      "929 ['cnnpolitics'] []\n",
      "0 0 1\n",
      "930 [] []\n",
      "0 0 0\n",
      "931 [] []\n",
      "0 0 0\n",
      "932 ['hotfreestyle', 'migos', 'culture'] ['migos', 'culture']\n",
      "2 0 1\n",
      "933 [] []\n",
      "0 0 0\n",
      "934 ['serina _ sgill', 'janet', 'stacy _ wahllove', 'donniewahlberg'] []\n",
      "0 0 4\n",
      "935 [] []\n",
      "0 0 0\n",
      "936 ['no fun bar'] []\n",
      "0 0 1\n",
      "937 [] []\n",
      "0 0 0\n",
      "938 ['craigrbrittain'] []\n",
      "0 0 1\n",
      "939 ['advertise anything'] []\n",
      "0 0 1\n",
      "940 [] []\n",
      "0 0 0\n",
      "941 [] []\n",
      "0 0 0\n",
      "942 ['soompi', 'lee joon gi', 'iu'] ['lee joon gi', 'iu']\n",
      "2 0 1\n",
      "943 ['gaviscon'] []\n",
      "0 0 1\n",
      "944 ['airbnb', 'airbnb'] []\n",
      "0 0 2\n",
      "945 [] []\n",
      "0 0 0\n",
      "946 ['hugotdre'] []\n",
      "0 0 1\n",
      "947 [] []\n",
      "0 0 0\n",
      "948 ['ellsavelli'] ['ellsavelli']\n",
      "1 0 0\n",
      "949 ['beatfaceleah'] []\n",
      "0 0 1\n",
      "950 ['briankoppelman', 'jaketapper', 'jak'] ['briankoppelman']\n",
      "1 0 2\n",
      "951 ['the black keys', 'lonely boy'] ['the black keys', 'lonely boy']\n",
      "2 0 0\n",
      "952 ['stephenjeske', 'quora'] ['stephenjeske']\n",
      "1 0 1\n",
      "953 ['kragar'] ['kragar']\n",
      "1 0 0\n",
      "954 [] []\n",
      "0 0 0\n",
      "955 ['linux os', 'ubuntu'] []\n",
      "0 0 2\n",
      "956 ['maxinejiji'] []\n",
      "0 0 1\n",
      "957 ['jzaffos', 'highcountrynews'] []\n",
      "0 0 2\n",
      "958 [] []\n",
      "0 0 0\n",
      "959 ['five guys'] []\n",
      "0 0 1\n",
      "960 ['catalonia', 'raulromeva'] ['catalonia']\n",
      "1 0 1\n",
      "961 [] []\n",
      "0 0 0\n",
      "962 [] []\n",
      "0 0 0\n",
      "963 ['gaypridebelize', 'missuniverse', 'belize', 'belize'] ['let']\n",
      "0 1 4\n",
      "964 [] []\n",
      "0 0 0\n",
      "965 ['biggdawg c loc'] ['biggdawg', 'c loc']\n",
      "0 2 1\n",
      "966 ['virgo'] []\n",
      "0 0 1\n",
      "967 [] []\n",
      "0 0 0\n",
      "968 [] []\n",
      "0 0 0\n",
      "969 ['kleinisd', 'kleinisd daily'] []\n",
      "0 0 2\n",
      "970 [] []\n",
      "0 0 0\n",
      "971 [] []\n",
      "0 0 0\n",
      "972 ['geologists', 'physicists', 'chemists', 'anthropologists', 'geneticists', 'epa'] []\n",
      "0 0 6\n",
      "973 [] ['vicmensa']\n",
      "0 1 0\n",
      "974 ['cheney'] ['cheney']\n",
      "1 0 0\n",
      "975 ['katherllne'] []\n",
      "0 0 1\n",
      "976 [] []\n",
      "0 0 0\n",
      "977 ['napa'] ['napa']\n",
      "1 0 0\n",
      "978 ['cloyd rivers', 'dory'] ['cloyd', 'rivers account']\n",
      "0 2 2\n",
      "979 [] []\n",
      "0 0 0\n",
      "980 [] []\n",
      "0 0 0\n",
      "981 ['power 95 . 3'] []\n",
      "0 0 1\n",
      "982 ['rick warren'] ['rick warren']\n",
      "1 0 0\n",
      "983 ['kathlyn'] ['kathlyn']\n",
      "1 0 0\n",
      "984 ['marcusb', 'richhomiehuang', 'alan'] ['marcusb', 'alan']\n",
      "2 0 1\n",
      "985 [] []\n",
      "0 0 0\n",
      "986 ['hardcountfox'] []\n",
      "0 0 1\n",
      "987 [] []\n",
      "0 0 0\n",
      "988 ['seanyreidy'] ['seanyreidy']\n",
      "1 0 0\n",
      "989 ['2000 ad at the cartoon museum'] []\n",
      "0 0 1\n",
      "990 ['davidfrum', 'united states'] ['united', 'states']\n",
      "0 2 2\n",
      "991 [] []\n",
      "0 0 0\n",
      "992 [] []\n",
      "0 0 0\n",
      "993 ['dylpickle 13'] []\n",
      "0 0 1\n",
      "994 ['great manicure'] []\n",
      "0 0 1\n",
      "995 ['bosstips'] []\n",
      "0 0 1\n",
      "996 [] []\n",
      "0 0 0\n",
      "997 ['chiquita'] ['chiquita']\n",
      "1 0 0\n",
      "998 ['ziglar'] []\n",
      "0 0 1\n",
      "999 [] []\n",
      "0 0 0\n",
      "1000 [] []\n",
      "0 0 0\n",
      "1001 ['fortunebot'] []\n",
      "0 0 1\n",
      "1002 [] []\n",
      "0 0 0\n",
      "1003 [] []\n",
      "0 0 0\n",
      "1004 ['questions not answers', 'box of crayons'] []\n",
      "0 0 2\n",
      "1005 ['aladdin'] ['aladdin']\n",
      "1 0 0\n",
      "1006 [] []\n",
      "0 0 0\n",
      "1007 ['taehyung', 'jungkook'] ['taehyung', 'jungkook']\n",
      "2 0 0\n",
      "1008 ['trump', 'appalachia', 'allies'] ['nazaire', 'trump', 'appalachia']\n",
      "2 1 1\n",
      "1009 ['horizons'] []\n",
      "0 0 1\n",
      "1010 [] []\n",
      "0 0 0\n",
      "1011 ['lee young ae', 'saimdang', \"light ' s diary\"] ['lee young ae', 'saimdang']\n",
      "2 0 1\n",
      "1012 [] []\n",
      "0 0 0\n",
      "1013 [] []\n",
      "0 0 0\n",
      "1014 [] []\n",
      "0 0 0\n",
      "1015 [] []\n",
      "0 0 0\n",
      "1016 ['katarina _ neko'] ['katarina']\n",
      "0 1 1\n",
      "1017 ['davidlongoria', 'petteri tarkkonen'] ['davidlongoria', 'petteri tarkkonen']\n",
      "2 0 0\n",
      "1018 ['sims 4'] []\n",
      "0 0 1\n",
      "1019 [] []\n",
      "0 0 0\n",
      "1020 [] ['layla']\n",
      "0 1 0\n",
      "1021 ['uroxatral'] []\n",
      "0 0 1\n",
      "1022 [] ['black', 'gandalf']\n",
      "0 2 0\n",
      "1023 ['socal'] ['socal']\n",
      "1 0 0\n",
      "1024 ['kinkshamer'] []\n",
      "0 0 1\n",
      "1025 ['familia qian jung'] ['qian jung']\n",
      "0 1 1\n",
      "1026 [] []\n",
      "0 0 0\n",
      "1027 ['cnn'] []\n",
      "0 0 1\n",
      "1028 [] []\n",
      "0 0 0\n",
      "1029 ['raoul', 'madam mina'] ['raoul']\n",
      "1 0 1\n",
      "1030 [] []\n",
      "0 0 0\n",
      "1031 ['ed', 'maywardforcocacolaph'] ['ed']\n",
      "1 0 1\n",
      "1032 ['mariolopezextra', 'mamajune', 'wetv', 'jessicarabbit'] ['mariolopezextra', 'jessicarabbit']\n",
      "2 0 2\n",
      "1033 ['howdoyoufeelchallenge'] []\n",
      "0 0 1\n",
      "1034 [] []\n",
      "0 0 0\n",
      "1035 ['glasgow'] ['partly']\n",
      "0 1 1\n",
      "1036 ['maria', 'maria'] []\n",
      "0 0 2\n",
      "1037 ['realdonaldtrump'] []\n",
      "0 0 1\n",
      "1038 [] []\n",
      "0 0 0\n",
      "1039 ['notebook'] []\n",
      "0 0 1\n",
      "1040 [] []\n",
      "0 0 0\n",
      "1041 ['mariboros'] []\n",
      "0 0 1\n",
      "1042 [] []\n",
      "0 0 0\n",
      "1043 [\"money ain ' t a problem\", 'french montana', 'diddy'] ['french montana', 'diddy']\n",
      "2 0 1\n",
      "1044 ['50 cent', '21 questions', 'nate dogg'] ['50', 'cent', 'nate dogg']\n",
      "1 2 2\n",
      "1045 ['mirandabru'] []\n",
      "0 0 1\n",
      "1046 [] []\n",
      "0 0 0\n",
      "1047 ['lindsay'] ['lindsay']\n",
      "1 0 0\n",
      "1048 [] []\n",
      "0 0 0\n",
      "1049 [] []\n",
      "0 0 0\n",
      "1050 ['swallavideo'] []\n",
      "0 0 1\n",
      "1051 ['brooke', 'candy girls'] ['brooke', 'candy']\n",
      "1 1 1\n",
      "1052 ['bellarmine'] ['bellarmine']\n",
      "1 0 0\n",
      "1053 [] []\n",
      "0 0 0\n",
      "1054 ['up and vanished'] []\n",
      "0 0 1\n",
      "1055 ['auto dj show'] []\n",
      "0 0 1\n",
      "1056 ['jack gilinsky'] ['jack gilinsky']\n",
      "1 0 0\n",
      "1057 [] []\n",
      "0 0 0\n",
      "1058 ['moonlight', 'magnolias', 'fly community theater'] ['moonlight', 'magnolias']\n",
      "2 0 1\n",
      "1059 ['ttuck _ 5', 'mitch'] ['mitch']\n",
      "1 0 1\n",
      "1060 ['konnan 5150'] ['konnan']\n",
      "0 1 1\n",
      "1061 ['leicester', 'europa'] ['leicester', 'man']\n",
      "1 1 1\n",
      "1062 [] []\n",
      "0 0 0\n",
      "1063 [] []\n",
      "0 0 0\n",
      "1064 [] []\n",
      "0 0 0\n",
      "1065 [] []\n",
      "0 0 0\n",
      "1066 ['h 1 z 1', 'battlefield 1'] ['trials', 'with']\n",
      "0 2 2\n",
      "1067 ['iarmys'] []\n",
      "0 0 1\n",
      "1068 ['andrew mccabe', 'flynn', 'trump', 'google search', 'stefanmolyneux', 'realdonaldtrump', 'donaldjtrumpjr'] ['andrew mccabe', 'flynn', 'stefanmolyneux realdonaldtrump', 'donaldjtrumpjr @url']\n",
      "2 2 5\n",
      "1069 [] []\n",
      "0 0 0\n",
      "1070 ['mets'] ['mets']\n",
      "1 0 0\n",
      "1071 [] []\n",
      "0 0 0\n",
      "1072 ['emilia clarke'] ['emilia clarke']\n",
      "1 0 0\n",
      "1073 [] []\n",
      "0 0 0\n",
      "1074 [] []\n",
      "0 0 0\n",
      "1075 ['microgravitylab', 'foxy'] ['foxy']\n",
      "1 0 1\n",
      "1076 ['uporntube 2'] []\n",
      "0 0 1\n",
      "1077 ['addam'] []\n",
      "0 0 1\n",
      "1078 ['freddyamazin'] []\n",
      "0 0 1\n",
      "1079 ['schofe', 'gogglebox', 'itv', 'hollywills'] ['schofe']\n",
      "1 0 3\n",
      "1080 ['kammbe'] []\n",
      "0 0 1\n",
      "1081 ['tonytgoodman', 'wholefoods', 'norahrab'] []\n",
      "0 0 3\n",
      "1082 ['kode action 12 news', 'kode 12'] []\n",
      "0 0 2\n",
      "1083 [] []\n",
      "0 0 0\n",
      "1084 ['ireland', 'ireland'] ['ireland', 'ireland']\n",
      "2 0 0\n",
      "1085 ['sunnnnnnnnnna'] []\n",
      "0 0 1\n",
      "1086 [] ['uc']\n",
      "0 1 0\n",
      "1087 [] []\n",
      "0 0 0\n",
      "1088 ['elviclothing', \"into the wild ' collection\"] ['into wild collection']\n",
      "0 1 2\n",
      "1089 [] []\n",
      "0 0 0\n",
      "1090 ['race recap'] []\n",
      "0 0 1\n",
      "1091 [] []\n",
      "0 0 0\n",
      "1092 ['craptaxidermy', 'doreen'] ['doreen']\n",
      "1 0 1\n",
      "1093 ['mariboros'] []\n",
      "0 0 1\n",
      "1094 [] []\n",
      "0 0 0\n",
      "1095 [] []\n",
      "0 0 0\n",
      "1096 ['reborn', 'jane ederlyn'] ['jane ederlyn']\n",
      "1 0 1\n",
      "1097 [] []\n",
      "0 0 0\n",
      "1098 [] []\n",
      "0 0 0\n",
      "1099 ['depcrusaderz'] []\n",
      "0 0 1\n",
      "1100 ['birmingham', 'islamic', 'state'] ['birmingham']\n",
      "1 0 2\n",
      "1101 [] []\n",
      "0 0 0\n",
      "1102 [] []\n",
      "0 0 0\n",
      "1103 ['harry potter'] ['harry potter']\n",
      "1 0 0\n",
      "1104 ['ilgdaily'] []\n",
      "0 0 1\n",
      "1105 ['baszmm'] []\n",
      "0 0 1\n",
      "1106 [] []\n",
      "0 0 0\n",
      "1107 [] []\n",
      "0 0 0\n",
      "1108 ['niall horan'] ['niall horan']\n",
      "1 0 0\n",
      "1109 ['phronesismusic', 'frankfurt radio big band', 'julargjazz', 'the behemoth', 'editionrecords'] ['frankfurt']\n",
      "0 1 5\n",
      "1110 [] []\n",
      "0 0 0\n",
      "1111 ['corporalmum'] []\n",
      "0 0 1\n",
      "1112 [] []\n",
      "0 0 0\n",
      "1113 ['lanadelrey', 'music to watch boys to'] ['lanadelrey']\n",
      "1 0 1\n",
      "1114 ['yasminyonis'] ['yasminyonis']\n",
      "1 0 0\n",
      "1115 [] []\n",
      "0 0 0\n",
      "1116 ['anisasx'] []\n",
      "0 0 1\n",
      "1117 ['paudybala'] []\n",
      "0 0 1\n",
      "1118 ['rickygervais'] ['rickygervais']\n",
      "1 0 0\n",
      "1119 [] []\n",
      "0 0 0\n",
      "1120 ['world'] []\n",
      "0 0 1\n",
      "1121 ['miami dolphins', 'dion jordan'] ['miami', 'dolphins', 'dion jordan']\n",
      "1 2 1\n",
      "1122 [] []\n",
      "0 0 0\n",
      "1123 [] []\n",
      "0 0 0\n",
      "1124 ['jimmy fallon', 'blake shelton', 'uganda'] ['jimmy fallon', 'blake shelton']\n",
      "2 0 1\n",
      "1125 [] []\n",
      "0 0 0\n",
      "1126 [] []\n",
      "0 0 0\n",
      "1127 ['jessiebelnap'] ['jessiebelnap']\n",
      "1 0 0\n",
      "1128 [] []\n",
      "0 0 0\n",
      "1129 [] []\n",
      "0 0 0\n",
      "1130 ['parkwest', 'spring', 'tx'] ['parkwest', 'spring', 'tx']\n",
      "3 0 0\n",
      "1131 [] []\n",
      "0 0 0\n",
      "1132 [] []\n",
      "0 0 0\n",
      "1133 [] []\n",
      "0 0 0\n",
      "1134 ['pretape', 'richidenz', 'soundcloud'] ['richidenz']\n",
      "1 0 2\n",
      "1135 ['ivanka'] ['ivanka']\n",
      "1 0 0\n",
      "1136 [] []\n",
      "0 0 0\n",
      "1137 [] []\n",
      "0 0 0\n",
      "1138 [] []\n",
      "0 0 0\n",
      "1139 [] ['osborneresign']\n",
      "0 1 0\n",
      "1140 [] []\n",
      "0 0 0\n",
      "1141 ['mantan kekasih'] ['lyla']\n",
      "0 1 1\n",
      "1142 [] []\n",
      "0 0 0\n",
      "1143 ['shaq'] ['shaq']\n",
      "1 0 0\n",
      "1144 ['jinjjarevil'] []\n",
      "0 0 1\n",
      "1145 ['beauty and the beast'] []\n",
      "0 0 1\n",
      "1146 [] []\n",
      "0 0 0\n",
      "1147 ['drjimmystar'] []\n",
      "0 0 1\n",
      "1148 ['joyous'] []\n",
      "0 0 1\n",
      "1149 [] []\n",
      "0 0 0\n",
      "1150 [] []\n",
      "0 0 0\n",
      "1151 ['skai jackson', 'untitled magazine'] ['madammelanin', 'skai jackson']\n",
      "1 1 1\n",
      "1152 [] []\n",
      "0 0 0\n",
      "1153 [] []\n",
      "0 0 0\n",
      "1154 [] []\n",
      "0 0 0\n",
      "1155 ['apna sangeet', 'valeti bhabiyan'] []\n",
      "0 0 2\n",
      "1156 ['altyellonatpark'] []\n",
      "0 0 1\n",
      "1157 ['djs', 'soundcloud mixes'] []\n",
      "0 0 2\n",
      "1158 ['floral', 'kaleidoscope', 'kaye', 'menner'] ['menner', 'photography quality']\n",
      "1 1 3\n",
      "1159 ['jacobrhines'] ['jacobrhines']\n",
      "1 0 0\n",
      "1160 ['jacobwhitesides'] ['jacobwhitesides']\n",
      "1 0 0\n",
      "1161 ['mariboros'] []\n",
      "0 0 1\n",
      "1162 ['edsheeran'] ['edsheeran']\n",
      "1 0 0\n",
      "1163 ['itstravelvibes', 'birmingham', 'united kingdom'] ['birmingham', 'united', 'kingdom']\n",
      "1 2 2\n",
      "1164 ['helen'] []\n",
      "0 0 1\n",
      "1165 ['chikorita', 'paramount'] []\n",
      "0 0 2\n",
      "1166 ['cocacolaph'] ['cocacolaph', '721']\n",
      "1 1 0\n",
      "1167 ['huffpostweird'] []\n",
      "0 0 1\n",
      "1168 [] []\n",
      "0 0 0\n",
      "1169 [] []\n",
      "0 0 0\n",
      "1170 [] []\n",
      "0 0 0\n",
      "1171 ['jack gilinsky'] ['jack gilinsky']\n",
      "1 0 0\n",
      "1172 [] []\n",
      "0 0 0\n",
      "1173 [] []\n",
      "0 0 0\n",
      "1174 [] []\n",
      "0 0 0\n",
      "1175 [] []\n",
      "0 0 0\n",
      "1176 [] []\n",
      "0 0 0\n",
      "1177 ['ebook blitz : the eslites', 'cm doporto', 'ebblitz'] ['cm doporto']\n",
      "1 0 2\n",
      "1178 [] []\n",
      "0 0 0\n",
      "1179 [] []\n",
      "0 0 0\n",
      "1180 [] []\n",
      "0 0 0\n",
      "1181 ['age of defenders - multiplayer tower defense and offense', 'ipad'] ['of defenders']\n",
      "0 1 2\n",
      "1182 ['beautyandthebeast'] []\n",
      "0 0 1\n",
      "1183 ['isabella'] ['isabella']\n",
      "1 0 0\n",
      "1184 ['climb _ chairman'] []\n",
      "0 0 1\n",
      "1185 ['chabot', 'cameron'] ['cameron']\n",
      "1 0 1\n",
      "1186 ['keselowski', 'fordperformance'] ['keselowski']\n",
      "1 0 1\n",
      "1187 [] []\n",
      "0 0 0\n",
      "1188 ['teen wolf'] []\n",
      "0 0 1\n",
      "1189 ['og mandino'] ['og mandino']\n",
      "1 0 0\n",
      "1190 [] []\n",
      "0 0 0\n",
      "1191 ['aries'] []\n",
      "0 0 1\n",
      "1192 ['juniorcyclingmy'] []\n",
      "0 0 1\n",
      "1193 ['grimm', 'nbc'] ['nbcgrimm', 'grimm', 'nbc']\n",
      "2 1 0\n",
      "1194 [] []\n",
      "0 0 0\n",
      "1195 [] ['stefondiggs']\n",
      "0 1 0\n",
      "1196 ['trump team'] ['team']\n",
      "0 1 1\n",
      "1197 ['ganaconwhoolist'] []\n",
      "0 0 1\n",
      "1198 [] []\n",
      "0 0 0\n",
      "1199 ['hozay', 'olive garden'] ['instead @url']\n",
      "0 1 2\n",
      "1200 [] []\n",
      "0 0 0\n",
      "1201 ['amitabh', 'shweta'] ['amitabh', 'shweta']\n",
      "2 0 0\n",
      "1202 ['naked city tv series'] ['city']\n",
      "0 1 1\n",
      "1203 [] []\n",
      "0 0 0\n",
      "1204 [] []\n",
      "0 0 0\n",
      "1205 ['bitter'] []\n",
      "0 0 1\n",
      "1206 ['dinko'] ['dinko']\n",
      "1 0 0\n",
      "1207 [] []\n",
      "0 0 0\n",
      "1208 ['madama butterfly'] ['madama']\n",
      "0 1 1\n",
      "1209 [] []\n",
      "0 0 0\n",
      "1210 [] []\n",
      "0 0 0\n",
      "1211 [] []\n",
      "0 0 0\n",
      "1212 ['amber', 'elyxx'] ['amber']\n",
      "1 0 1\n",
      "1213 ['klathelyricist'] []\n",
      "0 0 1\n",
      "1214 ['lil yachty'] ['lil yachty']\n",
      "1 0 0\n",
      "1215 ['trumpie'] ['trumpie']\n",
      "1 0 0\n",
      "1216 [] []\n",
      "0 0 0\n",
      "1217 [] []\n",
      "0 0 0\n",
      "1218 ['nprmusic', 'chicanobatman', 'spoontheband', 'thevaleriejune', 'sxsw'] []\n",
      "0 0 5\n",
      "1219 [] []\n",
      "0 0 0\n",
      "1220 ['sheilagunnreid', 'therebeltv'] ['therebeltv']\n",
      "1 0 1\n",
      "1221 [] []\n",
      "0 0 0\n",
      "1222 ['jack pearson', 'rebecca'] ['jack pearson']\n",
      "1 0 1\n",
      "1223 ['charleston', 'ers 2017'] ['twentyonepilots', 'charleston']\n",
      "1 1 1\n",
      "1224 [] []\n",
      "0 0 0\n",
      "1225 ['flirtynotes'] []\n",
      "0 0 1\n",
      "1226 ['cbssports'] ['i']\n",
      "0 1 1\n",
      "1227 ['mark', 'boy scouts'] []\n",
      "0 0 2\n",
      "1228 ['tatum'] ['tatum', 'melo']\n",
      "1 1 0\n",
      "1229 [] []\n",
      "0 0 0\n",
      "1230 ['jackiedomingez'] []\n",
      "0 0 1\n",
      "1231 ['hugo', 'hugo'] ['hugo']\n",
      "1 0 1\n",
      "1232 ['tiger mama'] []\n",
      "0 0 1\n",
      "1233 [] []\n",
      "0 0 0\n",
      "1234 ['nexus 6 p', 'nexus 6 sleeve', 'nexus 6', 'nexus 6', 'nexus 6 psleeve', 'nexus 6 pcover', 'nexus 6 pleather', 'nexus 6 pcase'] []\n",
      "0 0 8\n",
      "1235 [] []\n",
      "0 0 0\n",
      "1236 ['trump administration'] []\n",
      "0 0 1\n",
      "1237 ['russ', 'rhodeisland'] ['russ', 'rhodeisland']\n",
      "2 0 0\n",
      "1238 [] []\n",
      "0 0 0\n",
      "1239 ['cargoindustry'] []\n",
      "0 0 1\n",
      "1240 [] []\n",
      "0 0 0\n",
      "1241 [] []\n",
      "0 0 0\n",
      "1242 ['salhernandez', 'brett baier', 'fox news', 'andrew napolitano', 'fox news'] ['salhernandez', 'brett baier', 'fox news', 'andrew napolitano', 'not able']\n",
      "4 1 1\n",
      "1243 [] []\n",
      "0 0 0\n",
      "1244 [] []\n",
      "0 0 0\n",
      "1245 ['alain', 'plague inc : evolved', 'ps 4'] ['alain', 'plague inc : evolved', '4']\n",
      "2 1 1\n",
      "1246 ['bubblestbh', 'instagr'] []\n",
      "0 0 2\n",
      "1247 [] []\n",
      "0 0 0\n",
      "1248 ['curious george'] ['curious george']\n",
      "1 0 0\n",
      "1249 [] []\n",
      "0 0 0\n",
      "1250 ['fordm', 'kellyanne conway'] ['kellyanne conway']\n",
      "1 0 1\n",
      "1251 ['emmawillis'] ['emmawillis']\n",
      "1 0 0\n",
      "1252 [] []\n",
      "0 0 0\n",
      "1253 ['white house'] []\n",
      "0 0 1\n",
      "1254 [] []\n",
      "0 0 0\n",
      "1255 [] []\n",
      "0 0 0\n",
      "1256 [] []\n",
      "0 0 0\n",
      "1257 [] []\n",
      "0 0 0\n",
      "1258 ['europe', 'europe', 'white', 'house'] ['s']\n",
      "0 1 4\n",
      "1259 ['elklien', 'kat slater'] ['slater having']\n",
      "0 1 2\n",
      "1260 [] []\n",
      "0 0 0\n",
      "1261 ['yoshkumar', 'esports'] ['yoshkumar']\n",
      "1 0 1\n",
      "1262 ['kim burrel'] ['kim burrel']\n",
      "1 0 0\n",
      "1263 [] []\n",
      "0 0 0\n",
      "1264 ['boyband _ joao', 'joão', 'teamjoa'] ['joao', 'joão']\n",
      "1 1 2\n",
      "1265 [] []\n",
      "0 0 0\n",
      "1266 ['postmates'] ['postmates']\n",
      "1 0 0\n",
      "1267 ['drebae'] []\n",
      "0 0 1\n",
      "1268 [] []\n",
      "0 0 0\n",
      "1269 [] []\n",
      "0 0 0\n",
      "1270 ['anthonyujr'] []\n",
      "0 0 1\n",
      "1271 [] ['thecourtkim']\n",
      "0 1 0\n",
      "1272 ['wildfire', 'lfdhcom'] []\n",
      "0 0 2\n",
      "1273 ['rhettandlink'] []\n",
      "0 0 1\n",
      "1274 ['richmonddoc ty'] []\n",
      "0 0 1\n",
      "1275 [] []\n",
      "0 0 0\n",
      "1276 ['archie', 'blair'] ['litgrimes', 'archie', 'blair']\n",
      "2 1 0\n",
      "1277 ['mariboros'] []\n",
      "0 0 1\n",
      "1278 [] []\n",
      "0 0 0\n",
      "1279 [] []\n",
      "0 0 0\n",
      "1280 ['shakers'] []\n",
      "0 0 1\n",
      "1281 [] []\n",
      "0 0 0\n",
      "1282 [] []\n",
      "0 0 0\n",
      "1283 [] []\n",
      "0 0 0\n",
      "1284 [] []\n",
      "0 0 0\n",
      "1285 ['cousins', 'pelicans', 'holiday'] ['cousins', 'pelicans', 'holiday']\n",
      "3 0 0\n",
      "1286 [] ['kenyeahmonae']\n",
      "0 1 0\n",
      "true_positive_count,false_positive_count,false_negative_count:\n",
      "385 233 692\n",
      "========Entity Mention Detection========\n",
      "precision:  0.6229773462783171\n",
      "recall:  0.3574744661095636\n",
      "f_measure:  0.45427728613569324\n",
      "========Entity Detection========\n",
      "precision:  0.3694951664876477\n",
      "recall:  0.6220614828209765\n",
      "f_measure:  0.46361185983827496\n"
     ]
    }
   ],
   "source": [
    "calculate_f1(tweet_to_sentences_w_annotation, tokenized_sentences, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRvhsCdn-NxN"
   },
   "source": [
    "# **Extracting Embeddings for TRAINING the STS Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STS_Training_embeddings(testset):\n",
    "    predictions=[]\n",
    "    tokenized_sentences=[]\n",
    "    count=0\n",
    "    entity_embeddings=[]\n",
    "    with torch.no_grad():\n",
    "        for test_record in testset:\n",
    "            # print(test_record)\n",
    "            # test_record=test_record.lower()\n",
    "            tokenized_input=tokenizer(test_record)\n",
    "            initial_input_ids = torch.tensor([tokenizer.encode(test_record)])\n",
    "            token_dict = {x : tokenizer.encode(x, add_special_tokens=False) for x in test_record.split()}\n",
    "            input_ids = initial_input_ids.to(device)\n",
    "            tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "\n",
    "            # output = model(input_ids)\n",
    "\n",
    "            output = alt_model(input_ids)\n",
    "            token_embeddings=output.hidden_states[-2].squeeze()[1:-1] # we dont need embeddings for CLS and EOS\n",
    "\n",
    "\n",
    "            prediction = (torch.argmax(output.logits, axis=2))\n",
    "            prediction = prediction.cpu().numpy().reshape(-1)\n",
    "            prediction_labels=[label_list[l].split('-')[0] for l in prediction]\n",
    "            \n",
    "#             print(token_embeddings.shape)\n",
    "            prediction_labels, entity_aware_embeddings=collate_token_label_embedding(token_dict, prediction_labels[1:-1],token_embeddings)\n",
    "            \n",
    "            predictions.append(prediction_labels)\n",
    "            entity_embeddings.append(entity_aware_embeddings)\n",
    "            tokenized_sentences.append(token_dict.keys())\n",
    "            \n",
    "            assert (len(prediction_labels)==len(token_dict.keys()))\n",
    "            assert (len(entity_aware_embeddings)==len(token_dict.keys()))\n",
    "\n",
    "#             if(count<=5):\n",
    "#                 print(len(token_dict.keys()),':', token_dict.keys())\n",
    "#                 print(len(initial_input_ids),':',initial_input_ids)\n",
    "#                 print(len(input_ids),':',input_ids)\n",
    "#                 print(len(entity_aware_embeddings))\n",
    "#                 print(len(prediction_labels),':',prediction_labels)\n",
    "#                 print('===============')\n",
    "#             count+=1\n",
    "\n",
    "    print(len(predictions),len(tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_NER_embeddings(testset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "UFKMsh5Arns8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['sentence1', 'sentence2', 'score'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "stsTrainDataDictList=[]\n",
    "stsTrainData_columns =['sentence1', 'sentence2', 'score']\n",
    "f=open(\"data/stsbenchmark/sts-train.csv\",'r')\n",
    "file_text= f.read()\n",
    "lines=file_text.split('\\n')\n",
    "for line in lines:\n",
    "    if(line):\n",
    "        fields=line.split('\\t')\n",
    "#         print(len(fields))\n",
    "        dataDict={'sentence1':fields[5],'sentence2':fields[6],'score':fields[4]}\n",
    "#         print(dataDict)\n",
    "        stsTrainDataDictList.append(dataDict)\n",
    "stsTrainData=pd.DataFrame(stsTrainDataDictList)\n",
    "# stsTrainData.columns =['genre', 'filename', 'year', 'unidentified', 'score', 'sentence1', 'sentence2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5749\n"
     ]
    }
   ],
   "source": [
    "print(len(stsTrainData))\n",
    "print(stsTrainData.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 : dict_keys(['A', 'plane', 'is', 'taking', 'off.'])\n",
      "1 : tensor([[   0,  104, 3402,   17,  602,  142,    4,    2]])\n",
      "1 : tensor([[   0,  104, 3402,   17,  602,  142,    4,    2]], device='cuda:0')\n",
      "5\n",
      "5 : ['O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "7 : dict_keys(['A', 'man', 'is', 'playing', 'a', 'large', 'flute.'])\n",
      "1 : tensor([[    0,   104,   171,    17,   444,    11,  1858, 38831,     4,     2]])\n",
      "1 : tensor([[    0,   104,   171,    17,   444,    11,  1858, 38831,     4,     2]],\n",
      "       device='cuda:0')\n",
      "7\n",
      "7 : ['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "9 : dict_keys(['A', 'man', 'is', 'spreading', 'shreded', 'cheese', 'on', 'a', 'pizza.'])\n",
      "1 : tensor([[    0,   104,   171,    17,  7124, 51222,   800,  2652,    24,    11,\n",
      "          1983,     4,     2]])\n",
      "1 : tensor([[    0,   104,   171,    17,  7124, 51222,   800,  2652,    24,    11,\n",
      "          1983,     4,     2]], device='cuda:0')\n",
      "9\n",
      "9 : ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "5 : dict_keys(['Three', 'men', 'are', 'playing', 'chess.'])\n",
      "1 : tensor([[    0,  3340,   656,    41,   444, 16902,     4,     2]])\n",
      "1 : tensor([[    0,  3340,   656,    41,   444, 16902,     4,     2]],\n",
      "       device='cuda:0')\n",
      "5\n",
      "5 : ['O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "6 : dict_keys(['A', 'man', 'is', 'playing', 'the', 'cello.'])\n",
      "1 : tensor([[    0,   104,   171,    17,   444,     6, 43236,     4,     2]])\n",
      "1 : tensor([[    0,   104,   171,    17,   444,     6, 43236,     4,     2]],\n",
      "       device='cuda:0')\n",
      "6\n",
      "6 : ['O', 'O', 'O', 'O', 'O', 'O']\n",
      "===============\n",
      "4 : dict_keys(['Some', 'men', 'are', 'fighting.'])\n",
      "1 : tensor([[   0,  726,  656,   41, 1910,    4,    2]])\n",
      "1 : tensor([[   0,  726,  656,   41, 1910,    4,    2]], device='cuda:0')\n",
      "4\n",
      "4 : ['O', 'O', 'O', 'O']\n",
      "===============\n",
      "5749 5749\n"
     ]
    }
   ],
   "source": [
    "# first pass it through NER Engine and get the contextual embeddings\n",
    "\n",
    "#For Source Sentences:\n",
    "source_sentence_embeddings= get_STS_Training_embeddings(stsTrainData['sentence1'].tolist())\n",
    "\n",
    "#For Target Sentences:\n",
    "target_sentence_embeddings= get_STS_Training_embeddings(stsTrainData['sentence2'].tolist())\n",
    "\n",
    "assert len(source_sentence_embeddings)==len(target_sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "PvEaaQhsX8aG",
    "GCq9EEmjU1li"
   ],
   "machine_shape": "hm",
   "name": "BERTweet-CollectiveNER.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "16630435853948fb8577359df4fced07": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "3a0681b5ea5b4ec1903e7584079c1f5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5cf84f3ac4564b2da0435adbbfe742c4",
       "IPY_MODEL_e00e9ad5eb384df996b593e6f8880164"
      ],
      "layout": "IPY_MODEL_66e745e8a80545259a5e9bfaf82807a3"
     }
    },
    "48448e592c5b4d2190545276aa136f99": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4fc8c07114a4b658bb555c955334f19",
      "placeholder": "​",
      "style": "IPY_MODEL_99f43041f80e41bea13dd071f1c24a8b",
      "value": " 1009/1009 [00:03&lt;00:00, 333.19ex/s]"
     }
    },
    "485e8c2ad26145e68ae482d8206cf3f8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "56acf1016e8b40289555396d6ab3dee7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cf84f3ac4564b2da0435adbbfe742c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9f4914e1c3be4c258ac0d3b35a3fb2d9",
      "max": 1287,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_16630435853948fb8577359df4fced07",
      "value": 1287
     }
    },
    "66e745e8a80545259a5e9bfaf82807a3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f43041f80e41bea13dd071f1c24a8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9f4914e1c3be4c258ac0d3b35a3fb2d9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b67d2a27c9ad4b018903a0a7afde3a20": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c3e04128e7cd488e801c8a31b17edce3",
       "IPY_MODEL_48448e592c5b4d2190545276aa136f99"
      ],
      "layout": "IPY_MODEL_56acf1016e8b40289555396d6ab3dee7"
     }
    },
    "c3e04128e7cd488e801c8a31b17edce3": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_485e8c2ad26145e68ae482d8206cf3f8",
      "max": 1009,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc7665ab60314474b1b2f38639a8986c",
      "value": 1009
     }
    },
    "cc7665ab60314474b1b2f38639a8986c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "d4fc8c07114a4b658bb555c955334f19": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd181fcce48041f4b634d2e562bf3b43": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e00e9ad5eb384df996b593e6f8880164": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd181fcce48041f4b634d2e562bf3b43",
      "placeholder": "​",
      "style": "IPY_MODEL_f9dfec3fd9034e0e8ee23489f5331bbf",
      "value": " 1287/1287 [00:04&lt;00:00, 297.53ex/s]"
     }
    },
    "f9dfec3fd9034e0e8ee23489f5331bbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
